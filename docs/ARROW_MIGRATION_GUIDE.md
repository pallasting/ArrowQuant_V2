# Arrow é›¶æ‹·è´ä¼˜åŒ–è¿ç§»æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©æ‚¨å°†ç°æœ‰ä»£ç è¿ç§»åˆ° Arrow é›¶æ‹·è´ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå®ç° 10-20x æ€§èƒ½æå‡å’Œ 80% å†…å­˜èŠ‚çœã€‚

**è¿ç§»æ”¶ç›Š**:
- âœ… 10-64x æ€§èƒ½æå‡
- âœ… 76-80% å†…å­˜èŠ‚çœ
- âœ… æ”¯æŒ 100K+ è®°å¿†è§„æ¨¡
- âœ… å‘åå…¼å®¹ï¼ˆæ—§ä»£ç ç»§ç»­å·¥ä½œï¼‰

**è¿ç§»æˆæœ¬**:
- ğŸ”„ ä»£ç ä¿®æ”¹ï¼šæœ€å°ï¼ˆä¸»è¦æ˜¯æ–¹æ³•åå˜åŒ–ï¼‰
- ğŸ”„ å­¦ä¹ æ›²çº¿ï¼šä½ï¼ˆAPI è®¾è®¡ç›¸ä¼¼ï¼‰
- ğŸ”„ æµ‹è¯•å·¥ä½œï¼šä¸­ç­‰ï¼ˆéœ€è¦éªŒè¯åŠŸèƒ½ï¼‰

---

## å¿«é€Ÿå¼€å§‹

### 1. æœ€å°è¿ç§»ï¼ˆ5 åˆ†é’Ÿï¼‰

åªéœ€æ·»åŠ  `_arrow` åç¼€å³å¯ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼š

```python
# æ—§ä»£ç 
from llm_compression.embedder import LocalEmbedder

embedder = LocalEmbedder()
embeddings = embedder.encode_batch(texts)

# æ–°ä»£ç ï¼ˆé›¶æ‹·è´ä¼˜åŒ–ï¼‰
from llm_compression.embedder_arrow import LocalEmbedderArrow

embedder_arrow = LocalEmbedderArrow()
embeddings_array = embedder_arrow.batch_encode_arrow(texts)  # è¿”å› Arrow Array
```

### 2. å®Œæ•´è¿ç§»ï¼ˆ30 åˆ†é’Ÿï¼‰

è¿ç§»åˆ°å®Œæ•´çš„ Arrow æµæ°´çº¿ï¼š

```python
# æ—§ä»£ç 
from llm_compression.cognitive_loop import CognitiveLoop

loop = CognitiveLoop()
# ... æ·»åŠ è®°å¿† ...
result = await loop.process(query, query_embedding)

# æ–°ä»£ç ï¼ˆç«¯åˆ°ç«¯é›¶æ‹·è´ï¼‰
from llm_compression.cognitive_loop_arrow import CognitiveLoopArrow

loop_arrow = CognitiveLoopArrow()
# ... æ‰¹é‡æ·»åŠ è®°å¿†ï¼ˆé›¶æ‹·è´ï¼‰...
loop_arrow.batch_add_memories_arrow(memory_ids, contents)
result = await loop_arrow.process_arrow(query)  # è‡ªåŠ¨ç¼–ç æŸ¥è¯¢
```

---

## æ¨¡å—è¿ç§»æŒ‡å—

### ArrowStorage (Task 12.1)

#### æ—§ä»£ç 
```python
from llm_compression.storage import ArrowStorage

storage = ArrowStorage()
storage.save(table, "memories.parquet")
table = storage.load("memories.parquet")

# é€è¡Œå¤„ç†ï¼ˆæ…¢ï¼‰
for i in range(len(table)):
    row = table.slice(i, 1)
    embedding = row['embedding'][0].as_py()  # æ•°æ®å¤åˆ¶ï¼
    # ... å¤„ç† ...
```

#### æ–°ä»£ç ï¼ˆé›¶æ‹·è´ï¼‰
```python
from llm_compression.arrow_storage_zero_copy import ArrowStorageZeroCopy
from llm_compression.arrow_zero_copy import ArrowBatchView, get_embeddings_buffer

storage = ArrowStorageZeroCopy()

# å†…å­˜æ˜ å°„åŠ è½½ï¼ˆé›¶æ‹·è´ï¼‰
table = storage.load_table_mmap("memories.parquet")

# æ–¹æ³• 1: æ‰¹é‡å¤„ç†ï¼ˆé›¶æ‹·è´ï¼‰
embeddings = get_embeddings_buffer(table, 'embedding')  # é›¶æ‹·è´æå–
# ... å‘é‡åŒ–å¤„ç† ...

# æ–¹æ³• 2: è¿­ä»£å¤„ç†ï¼ˆé›¶æ‹·è´ï¼‰
batch_view = ArrowBatchView(table)
for memory_view in batch_view:
    # å»¶è¿Ÿç‰©åŒ–ï¼Œåªåœ¨éœ€è¦æ—¶è½¬æ¢
    embedding = memory_view.get_numpy('embedding', zero_copy=True)
    # ... å¤„ç† ...
```

**æ€§èƒ½æå‡**: 16-64x

---

### LocalEmbedder (Task 12.2)

#### æ—§ä»£ç 
```python
from llm_compression.embedder import LocalEmbedder

embedder = LocalEmbedder()

# å•ä¸ªç¼–ç 
embedding = embedder.encode("text")

# æ‰¹é‡ç¼–ç 
embeddings = embedder.encode_batch(texts)  # è¿”å› NumPy æ•°ç»„

# ç›¸ä¼¼åº¦æœç´¢
similarities = embedder.similarity(query_vec, embeddings)
top_indices = np.argsort(similarities)[::-1][:top_k]
```

#### æ–°ä»£ç ï¼ˆArrow åŸç”Ÿï¼‰
```python
from llm_compression.embedder_arrow import LocalEmbedderArrow

embedder_arrow = LocalEmbedderArrow()

# å•ä¸ªç¼–ç ï¼ˆè¿”å› Arrow Arrayï¼‰
embedding_array = embedder_arrow.encode_to_arrow("text")

# æ‰¹é‡ç¼–ç ï¼ˆé›¶æ‹·è´ï¼‰
embeddings_array = embedder_arrow.batch_encode_arrow(texts)

# åˆ›å»º embedding è¡¨
embedding_table = embedder_arrow.create_embedding_table(
    texts=texts,
    include_text=True
)

# è¯­ä¹‰æœç´¢ï¼ˆé›¶æ‹·è´ï¼‰
result_table = embedder_arrow.semantic_search_arrow(
    query="search query",
    corpus_table=embedding_table,
    top_k=10
)

# æ‰¹é‡æœç´¢ï¼ˆå‘é‡åŒ–ï¼‰
results = embedder_arrow.batch_similarity_search(
    queries=["query1", "query2"],
    corpus_table=embedding_table,
    top_k=10
)
```

**æ€§èƒ½æå‡**: 2-10x

---

### NetworkNavigator (Task 12.3)

#### æ—§ä»£ç 
```python
from llm_compression.network_navigator import NetworkNavigator

navigator = NetworkNavigator()

# æ£€ç´¢ï¼ˆé€ä¸ªå¤„ç†ï¼‰
result = navigator.retrieve(
    query_embedding=query_vec,
    memory_network=memory_dict,
    max_results=10
)

# è®¿é—®è®°å¿†
for memory in result.memories:
    print(memory.content)
```

#### æ–°ä»£ç ï¼ˆå‘é‡åŒ–æ£€ç´¢ï¼‰
```python
from llm_compression.network_navigator_arrow import NetworkNavigatorArrow

navigator_arrow = NetworkNavigatorArrow()

# æ£€ç´¢ï¼ˆå‘é‡åŒ–ï¼Œé›¶æ‹·è´ï¼‰
result = navigator_arrow.retrieve_arrow(
    query_embedding=query_vec,
    memory_table=memory_table,  # Arrow Table
    max_results=10
)

# è®¿é—®è®°å¿†ï¼ˆé›¶æ‹·è´ï¼‰
memories_table = result.table
contents = memories_table['content'].to_pylist()

# ç®€åŒ–ç‰ˆç›¸ä¼¼åº¦æœç´¢ï¼ˆæ— æ¿€æ´»æ‰©æ•£ï¼‰
similar_table = navigator_arrow.find_similar_memories_vectorized(
    query_embedding=query_vec,
    memory_table=memory_table,
    top_k=10
)

# æ‰¹é‡æ£€ç´¢ï¼ˆå¹¶è¡Œï¼‰
results = navigator_arrow.batch_retrieve_arrow(
    query_embeddings=query_vecs,
    memory_table=memory_table,
    max_results=10
)
```

**æ€§èƒ½æå‡**: 16-20x

---

### CognitiveLoop (Task 12.5)

#### æ—§ä»£ç 
```python
from llm_compression.cognitive_loop import CognitiveLoop
from llm_compression.memory_primitive import MemoryPrimitive

loop = CognitiveLoop()

# æ·»åŠ è®°å¿†ï¼ˆé€ä¸ªï¼‰
for i, text in enumerate(texts):
    memory = MemoryPrimitive(
        id=f"mem{i}",
        content=text,
        embedding=embedder.encode(text)
    )
    loop.add_memory(memory)

# å¤„ç†æŸ¥è¯¢
query_embedding = embedder.encode(query)
result = await loop.process(query, query_embedding, max_memories=10)

print(result.output)
```

#### æ–°ä»£ç ï¼ˆç«¯åˆ°ç«¯é›¶æ‹·è´ï¼‰
```python
from llm_compression.cognitive_loop_arrow import CognitiveLoopArrow

loop_arrow = CognitiveLoopArrow()

# æ‰¹é‡æ·»åŠ è®°å¿†ï¼ˆé›¶æ‹·è´ï¼‰
loop_arrow.batch_add_memories_arrow(
    memory_ids=[f"mem{i}" for i in range(len(texts))],
    contents=texts
    # embeddings è‡ªåŠ¨ç¼–ç 
)

# æˆ–ä» Arrow Table åŠ è½½
loop_arrow.load_memories_from_table(memory_table)

# å¤„ç†æŸ¥è¯¢ï¼ˆè‡ªåŠ¨ç¼–ç ï¼‰
result = await loop_arrow.process_arrow(
    query=query,
    max_memories=10
)

print(result.output)
print(f"Processing time: {result.processing_time_ms:.1f}ms")

# æ‰¹é‡å¤„ç†æŸ¥è¯¢
results = await loop_arrow.batch_process_queries(
    queries=["query1", "query2", "query3"],
    max_memories=10
)
```

**æ€§èƒ½æå‡**: 10x ç«¯åˆ°ç«¯

---

## æ•°æ®æ ¼å¼è¿ç§»

### ä» Python å¯¹è±¡åˆ° Arrow Table

#### æ—§æ ¼å¼ï¼ˆPython å­—å…¸åˆ—è¡¨ï¼‰
```python
memories = [
    {
        'id': 'mem1',
        'content': 'Python is a programming language',
        'embedding': [0.1, 0.2, ...],
        'timestamp': 1234567890
    },
    # ...
]
```

#### æ–°æ ¼å¼ï¼ˆArrow Tableï¼‰
```python
import pyarrow as pa

# æ–¹æ³• 1: ä»å­—å…¸åˆ›å»º
memory_table = pa.table({
    'memory_id': pa.array(['mem1', 'mem2', ...]),
    'content': pa.array(['text1', 'text2', ...]),
    'embedding': embedder_arrow.batch_encode_arrow(texts),
    'timestamp': pa.array([1234567890, ...])
})

# æ–¹æ³• 2: ä½¿ç”¨ create_embedding_table
memory_table = embedder_arrow.create_embedding_table(
    texts=texts,
    include_text=True,
    additional_columns={
        'memory_id': memory_ids,
        'timestamp': timestamps
    }
)

# ä¿å­˜åˆ° Parquet
pa.parquet.write_table(memory_table, "memories.parquet")

# åŠ è½½ï¼ˆå†…å­˜æ˜ å°„ï¼Œé›¶æ‹·è´ï¼‰
from llm_compression.arrow_zero_copy import load_table_mmap
memory_table = load_table_mmap("memories.parquet")
```

---

## æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

### 1. ä½¿ç”¨æ‰¹é‡æ“ä½œ

âŒ **ä¸æ¨è**ï¼ˆé€ä¸ªå¤„ç†ï¼‰:
```python
for text in texts:
    embedding = embedder.encode(text)
    # ... å¤„ç† ...
```

âœ… **æ¨è**ï¼ˆæ‰¹é‡å¤„ç†ï¼‰:
```python
embeddings_array = embedder_arrow.batch_encode_arrow(texts, batch_size=32)
# ... å‘é‡åŒ–å¤„ç† ...
```

### 2. é¿å… .as_py() è°ƒç”¨

âŒ **ä¸æ¨è**ï¼ˆæ•°æ®å¤åˆ¶ï¼‰:
```python
for i in range(len(table)):
    content = table['content'][i].as_py()  # å¤åˆ¶ï¼
    embedding = table['embedding'][i].as_py()  # å¤åˆ¶ï¼
```

âœ… **æ¨è**ï¼ˆé›¶æ‹·è´ï¼‰:
```python
# æ‰¹é‡æå–
contents = table['content'].to_pylist()  # ä¸€æ¬¡æ€§è½¬æ¢
embeddings = get_embeddings_buffer(table, 'embedding')  # é›¶æ‹·è´

# æˆ–ä½¿ç”¨ ArrowBatchView
batch_view = ArrowBatchView(table)
for memory_view in batch_view:
    content = memory_view.get_py('content')  # å»¶è¿Ÿç‰©åŒ–
```

### 3. ä½¿ç”¨åˆ—è£å‰ª

âŒ **ä¸æ¨è**ï¼ˆåŠ è½½æ‰€æœ‰åˆ—ï¼‰:
```python
table = pa.parquet.read_table("memories.parquet")
# åªéœ€è¦ embedding åˆ—ï¼Œä½†åŠ è½½äº†æ‰€æœ‰åˆ—
```

âœ… **æ¨è**ï¼ˆåªåŠ è½½éœ€è¦çš„åˆ—ï¼‰:
```python
from llm_compression.arrow_zero_copy import prune_columns

table = pa.parquet.read_table(
    "memories.parquet",
    columns=['memory_id', 'embedding']  # åªåŠ è½½éœ€è¦çš„åˆ—
)
```

### 4. ä½¿ç”¨å†…å­˜æ˜ å°„

âŒ **ä¸æ¨è**ï¼ˆå…¨éƒ¨åŠ è½½åˆ°å†…å­˜ï¼‰:
```python
table = pa.parquet.read_table("large_memories.parquet")
```

âœ… **æ¨è**ï¼ˆå†…å­˜æ˜ å°„ï¼ŒæŒ‰éœ€åŠ è½½ï¼‰:
```python
from llm_compression.arrow_zero_copy import load_table_mmap

table = load_table_mmap("large_memories.parquet")  # æ”¯æŒ 10GB+ æ–‡ä»¶
```

### 5. ä½¿ç”¨å‘é‡åŒ–æ“ä½œ

âŒ **ä¸æ¨è**ï¼ˆPython å¾ªç¯ï¼‰:
```python
similarities = []
for embedding in embeddings:
    sim = np.dot(query_vec, embedding)
    similarities.append(sim)
```

âœ… **æ¨è**ï¼ˆå‘é‡åŒ–ï¼‰:
```python
from llm_compression.arrow_zero_copy import compute_similarity_zero_copy

similarities = compute_similarity_zero_copy(embeddings, query_vec)
```

---

## å¸¸è§é—®é¢˜

### Q1: æ—§ä»£ç è¿˜èƒ½ç”¨å—ï¼Ÿ

**A**: æ˜¯çš„ï¼æ‰€æœ‰æ—§ API ä¿æŒä¸å˜ï¼Œæ–°çš„ Arrow ä¼˜åŒ–æ˜¯å¯é€‰çš„ã€‚

```python
# æ—§ä»£ç ç»§ç»­å·¥ä½œ
from llm_compression.embedder import LocalEmbedder
embedder = LocalEmbedder()
embeddings = embedder.encode_batch(texts)

# æ–°ä»£ç æä¾›æ›´å¥½æ€§èƒ½
from llm_compression.embedder_arrow import LocalEmbedderArrow
embedder_arrow = LocalEmbedderArrow()
embeddings_array = embedder_arrow.batch_encode_arrow(texts)
```

### Q2: å¦‚ä½•åœ¨æ—§ä»£ç å’Œæ–°ä»£ç ä¹‹é—´è½¬æ¢ï¼Ÿ

**A**: ä½¿ç”¨ç®€å•çš„è½¬æ¢å‡½æ•°ï¼š

```python
import pyarrow as pa
import numpy as np

# NumPy â†’ Arrow
embeddings_np = np.array([[0.1, 0.2], [0.3, 0.4]])
embeddings_arrow = embedder_arrow._numpy_to_arrow_list(embeddings_np)

# Arrow â†’ NumPy
embeddings_np = get_embeddings_buffer(table, 'embedding')

# Arrow Table â†’ Pandas DataFrame
df = table.to_pandas()

# Pandas DataFrame â†’ Arrow Table
table = pa.Table.from_pandas(df)
```

### Q3: ä»€ä¹ˆæ—¶å€™åº”è¯¥è¿ç§»ï¼Ÿ

**A**: æ ¹æ®åœºæ™¯é€‰æ‹©ï¼š

| åœºæ™¯ | æ˜¯å¦è¿ç§» | åŸå›  |
|------|---------|------|
| è®°å¿†æ•° < 1K | å¯é€‰ | æ€§èƒ½æå‡ä¸æ˜æ˜¾ |
| è®°å¿†æ•° 1K-10K | æ¨è | 10-20x æ€§èƒ½æå‡ |
| è®°å¿†æ•° > 10K | å¼ºçƒˆæ¨è | å¿…éœ€ï¼Œå¦åˆ™å†…å­˜ä¸è¶³ |
| æ‰¹é‡å¤„ç† | æ¨è | æ˜¾è‘—æå‡ååé‡ |
| å®æ—¶æŸ¥è¯¢ | æ¨è | é™ä½å»¶è¿Ÿ |

### Q4: è¿ç§»ä¼šç ´åç°æœ‰æ•°æ®å—ï¼Ÿ

**A**: ä¸ä¼šã€‚Arrow å’Œ Parquet æ˜¯æ ‡å‡†æ ¼å¼ï¼Œå¯ä»¥ä¸ç°æœ‰å·¥å…·äº’æ“ä½œã€‚

```python
# æ—§æ•°æ®ï¼ˆParquetï¼‰
table_old = pa.parquet.read_table("old_memories.parquet")

# æ–°ä»£ç å¯ä»¥ç›´æ¥ä½¿ç”¨
loop_arrow.load_memories_from_table(table_old)
```

### Q5: å¦‚ä½•éªŒè¯è¿ç§»æ­£ç¡®æ€§ï¼Ÿ

**A**: ä½¿ç”¨å¯¹æ¯”æµ‹è¯•ï¼š

```python
# æ—§ä»£ç ç»“æœ
embeddings_old = embedder.encode_batch(texts)

# æ–°ä»£ç ç»“æœ
embeddings_array = embedder_arrow.batch_encode_arrow(texts)
embeddings_new = get_embeddings_buffer(
    pa.table({'embedding': embeddings_array}),
    'embedding'
)

# éªŒè¯ä¸€è‡´æ€§
np.testing.assert_allclose(embeddings_old, embeddings_new, rtol=1e-5)
```

---

## è¿ç§»æ£€æŸ¥æ¸…å•

### å‡†å¤‡é˜¶æ®µ
- [ ] é˜…è¯»æœ¬è¿ç§»æŒ‡å—
- [ ] äº†è§£ Arrow åŸºç¡€æ¦‚å¿µ
- [ ] å¤‡ä»½ç°æœ‰ä»£ç å’Œæ•°æ®

### è¿ç§»é˜¶æ®µ
- [ ] å®‰è£…ä¾èµ–ï¼š`pip install pyarrow`
- [ ] æ›´æ–°å¯¼å…¥è¯­å¥
- [ ] ä¿®æ”¹æ–¹æ³•è°ƒç”¨ï¼ˆæ·»åŠ  `_arrow` åç¼€ï¼‰
- [ ] è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆPython å¯¹è±¡ â†’ Arrow Tableï¼‰
- [ ] æ›´æ–°æµ‹è¯•ä»£ç 

### éªŒè¯é˜¶æ®µ
- [ ] è¿è¡Œå•å…ƒæµ‹è¯•
- [ ] å¯¹æ¯”æ–°æ—§ç»“æœä¸€è‡´æ€§
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] å†…å­˜ä½¿ç”¨åˆ†æ

### ä¼˜åŒ–é˜¶æ®µ
- [ ] åº”ç”¨æœ€ä½³å®è·µ
- [ ] ä½¿ç”¨æ‰¹é‡æ“ä½œ
- [ ] å¯ç”¨å†…å­˜æ˜ å°„
- [ ] åˆ—è£å‰ªä¼˜åŒ–

---

## ç¤ºä¾‹ï¼šå®Œæ•´è¿ç§»æµç¨‹

### æ­¥éª¤ 1: æ—§ä»£ç ï¼ˆåŸºçº¿ï¼‰

```python
from llm_compression.embedder import LocalEmbedder
from llm_compression.cognitive_loop import CognitiveLoop
from llm_compression.memory_primitive import MemoryPrimitive

# åˆå§‹åŒ–
embedder = LocalEmbedder()
loop = CognitiveLoop()

# æ·»åŠ è®°å¿†
texts = ["text1", "text2", "text3"]
for i, text in enumerate(texts):
    embedding = embedder.encode(text)
    memory = MemoryPrimitive(
        id=f"mem{i}",
        content=text,
        embedding=embedding
    )
    loop.add_memory(memory)

# å¤„ç†æŸ¥è¯¢
query = "search query"
query_embedding = embedder.encode(query)
result = await loop.process(query, query_embedding, max_memories=5)
```

### æ­¥éª¤ 2: è¿ç§»åˆ° Arrowï¼ˆä¼˜åŒ–ï¼‰

```python
from llm_compression.embedder_arrow import LocalEmbedderArrow
from llm_compression.cognitive_loop_arrow import CognitiveLoopArrow

# åˆå§‹åŒ–
embedder_arrow = LocalEmbedderArrow()
loop_arrow = CognitiveLoopArrow()

# æ‰¹é‡æ·»åŠ è®°å¿†ï¼ˆé›¶æ‹·è´ï¼‰
texts = ["text1", "text2", "text3"]
loop_arrow.batch_add_memories_arrow(
    memory_ids=[f"mem{i}" for i in range(len(texts))],
    contents=texts
)

# å¤„ç†æŸ¥è¯¢ï¼ˆè‡ªåŠ¨ç¼–ç ï¼‰
query = "search query"
result = await loop_arrow.process_arrow(query, max_memories=5)
```

### æ­¥éª¤ 3: éªŒè¯ç»“æœ

```python
# å¯¹æ¯”è¾“å‡º
print(f"Old output: {result_old.output}")
print(f"New output: {result_new.output}")

# å¯¹æ¯”æ€§èƒ½
print(f"Old time: {time_old:.1f}ms")
print(f"New time: {result_new.processing_time_ms:.1f}ms")
print(f"Speedup: {time_old / result_new.processing_time_ms:.1f}x")
```

---

## è·å–å¸®åŠ©

### æ–‡æ¡£èµ„æº
- `docs/ARROW_ZERO_COPY_OPTIMIZATION.md` - ä¼˜åŒ–æ–¹æ¡ˆè¯¦è§£
- `docs/ARROW_UNIFIED_PIPELINE.md` - ç»Ÿä¸€æµæ°´çº¿æ¶æ„
- `docs/ARROW_ZERO_COPY_USAGE.md` - ä½¿ç”¨æŒ‡å—
- `docs/TASK_12_FINAL_SUMMARY.md` - å®Œæ•´æ€»ç»“

### ä»£ç ç¤ºä¾‹
- `tests/unit/test_*_arrow.py` - å•å…ƒæµ‹è¯•ç¤ºä¾‹
- `tests/performance/test_*_benchmark.py` - æ€§èƒ½æµ‹è¯•ç¤ºä¾‹

### ç¤¾åŒºæ”¯æŒ
- GitHub Issues: æŠ¥å‘Šé—®é¢˜
- GitHub Discussions: æŠ€æœ¯è®¨è®º

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2026-02-17  
**é€‚ç”¨ç‰ˆæœ¬**: Phase 2.0 Task 12
