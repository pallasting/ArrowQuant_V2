# AI-OS ArrowEngine æ ¸å¿ƒè·¯å¾„å®æ–½è®¡åˆ’

## èƒŒæ™¯

ç»è¿‡å…¨é¢å®¡æŸ¥ï¼Œæˆ‘ä»¬ç¡®è®¤ ArrowEngine æ˜¯ AI-OS è®°å¿†ç³»ç»Ÿçš„**æ ¸å¿ƒè·¯å¾„**ã€‚å®ƒä¸ [ArrowStorage](file:///m:/Documents/ai-os-memory/llm_compression/arrow_storage.py#235-961) å…±äº« Arrow/Parquet æ•°æ®æ ¼å¼ï¼Œèƒ½å®ç°ä»æ¨¡å‹æƒé‡ â†’ åµŒå…¥æ¨ç† â†’ è®°å¿†å­˜å‚¨ â†’ è¯­ä¹‰æ£€ç´¢çš„**ç«¯åˆ°ç«¯é›¶æ‹·è´**ã€‚æœ¬è®¡åˆ’æ—¨åœ¨è¡¥å…¨ ArrowEngine å…³é”®åŸºç¡€è®¾æ–½ï¼ŒéªŒè¯æ ¸å¿ƒä»·å€¼ï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€æ¨è¿›æ•´ä¸ª AI-OS æ¶æ„ç»Ÿä¸€ã€‚

### å½“å‰çŠ¶æ€

| ç»„ä»¶ | å­˜åœ¨ | å®Œæˆåº¦ | é˜»å¡ç‚¹ |
|------|------|--------|--------|
| ModelConverter | âœ… | ~90% | â€” |
| ArrowEngine API | âœ… | ~80% | â€” |
| WeightLoader | âœ… | ~95% | â€” |
| FastTokenizer | âœ… | ~90% | â€” |
| **InferenceCore** | âš ï¸ | **~30%** | **æ—  Transformer å±‚ï¼Œä»… Embedding Lookup** |
| FastAPI Server | âœ… | ~85% | ç¼º Docker |
| 5 ä¸ªè¯­ä¹‰ç´¢å¼•æ¨¡å— | âŒ | 0% | æºæ–‡ä»¶ä¸å­˜åœ¨ |
| æ—§ embedder ä½“ç³» | âœ… | 100% | å†—ä½™ï¼Œéœ€è¿ç§» |

---

## Proposed Changes

### Phase 0: InferenceCore å®Œå–„ â€” æ¶ˆé™¤ç¬¬ä¸€é˜»å¡ç‚¹

> [!CAUTION]
> è¿™æ˜¯æ•´ä¸ªè®¡åˆ’çš„**å…³é”®è·¯å¾„**ã€‚InferenceCore å½“å‰çš„ [_forward_embeddings()](file:///m:/Documents/ai-os-memory/llm_compression/inference/inference_core.py#130-170) ä»…åš Embedding Lookupï¼ˆ40 è¡Œï¼‰ï¼Œæ²¡æœ‰å®ç° Transformer çš„å¤šå¤´æ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œå’Œå±‚å½’ä¸€åŒ–ã€‚æ²¡æœ‰è¿™äº›ï¼ŒArrowEngine çš„æ¨ç†ç²¾åº¦å°†è¿œä½äº sentence-transformersï¼Œæ•´ä¸ªæ›¿ä»£æ–¹æ¡ˆä¸æˆç«‹ã€‚

#### [MODIFY] [inference_core.py](file:///m:/Documents/ai-os-memory/llm_compression/inference/inference_core.py)

**å½“å‰é—®é¢˜**: [_forward_embeddings()](file:///m:/Documents/ai-os-memory/llm_compression/inference/inference_core.py#130-170) æ˜¯ä¸€ä¸ªç®€åŒ– stubï¼š

```python
# ç°çŠ¶: ä»…åš embedding lookupï¼Œæ—  transformer å±‚
hidden_states = torch.zeros(batch_size, seq_len, self.hidden_size, ...)
embedding_weight = None
for name in dir(self):
    if 'embedding' in name.lower() and 'weight' in name.lower():
        embedding_weight = getattr(self, name)
        break
```

**æ”¹é€ å†…å®¹**:

1. å®ç°å®Œæ•´çš„ BERT `TransformerLayer`ï¼ˆMulti-Head Self-Attention + FFN + LayerNormï¼‰
2. å‚ç…§ all-MiniLM-L6-v2 çš„æƒé‡ç»“æ„åŠ è½½é…ç½®ï¼ˆ6 å±‚ã€6 å¤´ã€384 ç»´ï¼‰
3. æ”¯æŒ `position_embeddings` å’Œ `token_type_embeddings`
4. æ·»åŠ  `@torch.no_grad()` æ¨ç†ä¼˜åŒ–

```diff
- def _forward_embeddings(self, input_ids, seq_len):
-     # ç®€åŒ– stub: ä»… embedding lookup
-     hidden_states = torch.zeros(...)
-     ...
-     return hidden_states

+ def _forward_embeddings(self, input_ids, attention_mask):
+     # å®Œæ•´ BERT å‰å‘ä¼ æ’­
+     # 1. Embedding Layer (word + position + token_type)
+     # 2. N x TransformerLayer (self-attention + FFN + LayerNorm)
+     # 3. è¿”å›æœ€ç»ˆéšè—çŠ¶æ€
```

**å…³é”®è®¾è®¡å†³ç­–**: ä¸ä¾èµ– `transformers` åº“ï¼Œå®Œå…¨è‡ªç ” Transformer å±‚ï¼Œä¿æŒ ArrowEngine çš„è½»é‡çº§ä¼˜åŠ¿ã€‚æƒé‡åç§°æ˜ å°„éœ€ä¸ [WeightLoader](file:///m:/Documents/ai-os-memory/llm_compression/inference/weight_loader.py#31-341) ä» Parquet åŠ è½½çš„æƒé‡é”®åä¸€è‡´ã€‚

**é¢„ä¼°å·¥æ—¶**: 12-16 å°æ—¶

---

#### [NEW] [tests/unit/inference/test_inference_core.py](file:///m:/Documents/ai-os-memory/tests/unit/inference/test_inference_core.py)

å•å…ƒæµ‹è¯•è¦†ç›–ï¼š
- `test_forward_output_shape` â€” è¾“å‡ºå½¢çŠ¶ [(batch_size, hidden_size)](file:///m:/Documents/ai-os-memory/llm_compression/client/client.py#195-220) æ­£ç¡®
- `test_mean_pooling` â€” Mean Pooling åœ¨æœ‰/æ—  mask æ—¶è¡Œä¸ºæ­£ç¡®
- `test_normalize_embeddings` â€” L2 å½’ä¸€åŒ–è¾“å‡ºå•ä½å‘é‡
- `test_transformer_layers_affect_output` â€” éªŒè¯ Transformer å±‚ç¡®å®æ”¹å˜äº† embedding lookup çš„è¾“å‡º
- `test_batch_consistency` â€” å•æ¡/æ‰¹é‡è¾“å‡ºä¸€è‡´

#### [NEW] [tests/unit/inference/test_weight_loader.py](file:///m:/Documents/ai-os-memory/tests/unit/inference/test_weight_loader.py)

- `test_load_weights_from_parquet` â€” Parquet â†’ Tensor è½¬æ¢æ­£ç¡®
- `test_memory_map_enabled` â€” Memory map æ¨¡å¼ä¸‹æ— æ•°æ®æ‹·è´
- `test_get_layer_lazy_loading` â€” æ‡’åŠ è½½å•å±‚æƒé‡

#### [NEW] [tests/unit/inference/test_arrow_engine.py](file:///m:/Documents/ai-os-memory/tests/unit/inference/test_arrow_engine.py)

- `test_encode_single_text` â€” å•æ–‡æœ¬ç¼–ç 
- `test_encode_batch` â€” æ‰¹é‡ç¼–ç å½¢çŠ¶æ­£ç¡®
- `test_similarity` â€” ç›¸ä¼¼åº¦è®¡ç®—ç»“æœåœ¨ [-1, 1] èŒƒå›´å†…
- `test_device_auto_detect` â€” è®¾å¤‡è‡ªåŠ¨æ£€æµ‹

---

### Phase 1: ç«¯åˆ°ç«¯éªŒè¯ â€” è¯æ˜æ ¸å¿ƒä»·å€¼

#### [NEW] [tests/integration/inference/test_e2e_precision.py](file:///m:/Documents/ai-os-memory/tests/integration/inference/test_e2e_precision.py)

ç«¯åˆ°ç«¯ç²¾åº¦éªŒè¯è„šæœ¬ï¼š

1. ç”¨ [ModelConverter](file:///m:/Documents/ai-os-memory/llm_compression/tools/model_converter.py#81-533) å°† `all-MiniLM-L6-v2` è½¬æ¢ä¸º Parquet æ ¼å¼
2. åˆ†åˆ«ç”¨ ArrowEngine å’Œ sentence-transformers ç¼–ç åŒä¸€ç»„æµ‹è¯•æ–‡æœ¬
3. è®¡ç®—ä¸¤è€…è¾“å‡ºçš„**é€ pair ä½™å¼¦ç›¸ä¼¼åº¦**
4. æ–­è¨€ç›¸ä¼¼åº¦ â‰¥ 0.99ï¼ˆlow bar: 0.95 ç®—é¢„è­¦ï¼Œ< 0.95 ç®—å¤±è´¥ï¼‰

```python
# éªŒè¯é€»è¾‘æ ¸å¿ƒ
def test_arrowengine_vs_sentence_transformers():
    """ArrowEngine è¾“å‡ºåº”ä¸ sentence-transformers é«˜åº¦ä¸€è‡´"""
    test_texts = [
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning is a subset of artificial intelligence.",
        "Python is a popular programming language.",
        # ... 20+ è¦†ç›–ä¸åŒé•¿åº¦å’Œä¸»é¢˜çš„æµ‹è¯•æ–‡æœ¬
    ]
    
    # ArrowEngine è·¯å¾„
    engine = ArrowEngine("./models/minilm")
    arrow_embeddings = engine.encode(test_texts)
    
    # sentence-transformers è·¯å¾„
    from sentence_transformers import SentenceTransformer
    st_model = SentenceTransformer("all-MiniLM-L6-v2")
    st_embeddings = st_model.encode(test_texts)
    
    # é€ pair æ¯”è¾ƒ
    for i in range(len(test_texts)):
        similarity = cosine_similarity(arrow_embeddings[i], st_embeddings[i])
        assert similarity >= 0.99, f"Text {i}: similarity={similarity:.4f}"
```

#### [NEW] [benchmarks/arrowengine_benchmark.py](file:///m:/Documents/ai-os-memory/benchmarks/arrowengine_benchmark.py)

æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¯” ArrowEngine vs sentence-transformersï¼š

| æŒ‡æ ‡ | ArrowEngine ç›®æ ‡ | ST åŸºçº¿ |
|------|-----------------|---------|
| æ¨¡å‹åŠ è½½æ—¶é—´ | < 100ms | 2-5s |
| å•æ¬¡æ¨ç†å»¶è¿Ÿ | < 5ms | 10-20ms |
| æ‰¹å¤„ç†ååé‡ | > 2000 req/s | 500-800 req/s |
| å†…å­˜å ç”¨ | < 100MB | ~180MB |

---

### Phase 2: ArrowEngine + ArrowStorage ç»Ÿä¸€æ•°æ®è·¯å¾„

#### [NEW] [llm_compression/embedding_provider.py](file:///m:/Documents/ai-os-memory/llm_compression/embedding_provider.py)

åˆ›å»ºç»Ÿä¸€çš„åµŒå…¥æ¥å£ï¼Œä½œä¸ºä¸‹æ¸¸æ¨¡å—å’Œåº•å±‚å¼•æ“ä¹‹é—´çš„æ¡¥æ¥å±‚ï¼š

```python
class EmbeddingProvider(Protocol):
    """ç»Ÿä¸€åµŒå…¥æ¥å£ â€” æ‰€æœ‰ä¸‹æ¸¸æ¨¡å—é€šè¿‡æ­¤æ¥å£è·å–åµŒå…¥"""
    
    def encode(self, text: str, normalize: bool = True) -> np.ndarray: ...
    def encode_batch(self, texts: List[str], ...) -> np.ndarray: ...
    def similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float: ...
    def get_embedding_dimension(self) -> int: ...

class ArrowEngineProvider(EmbeddingProvider):
    """åŸºäº ArrowEngine çš„å®ç°ï¼ˆæ¨èã€é»˜è®¤ï¼‰"""
    def __init__(self, model_path: str = "./models/minilm"): ...

class SentenceTransformerProvider(EmbeddingProvider):
    """åŸºäº sentence-transformers çš„åå¤‡å®ç°"""
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"): ...

def get_default_provider() -> EmbeddingProvider:
    """è·å–é»˜è®¤åµŒå…¥æä¾›è€…ï¼ˆä¼˜å…ˆ ArrowEngineï¼‰"""
```

**è®¾è®¡ç†ç”±**: é€šè¿‡ Protocol æ¥å£è§£è€¦ï¼Œä¸‹æ¸¸æ¨¡å—ä¸å†ç›´æ¥ä¾èµ– [LocalEmbedder](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#31-306) æˆ– [ArrowEngine](file:///m:/Documents/ai-os-memory/llm_compression/inference/arrow_engine.py#31-354)ï¼Œè¿ç§»æ—¶åªéœ€æ›¿æ¢ provider å®ä¾‹è€Œæ— éœ€æ”¹åŠ¨ä¸šåŠ¡é€»è¾‘ã€‚åŒæ—¶ä¿ç•™ sentence-transformers åå¤‡ï¼Œç¡®ä¿è¿ç§»æœŸé—´å¹³æ»‘è¿‡æ¸¡ã€‚

#### [MODIFY] [arrow_storage.py](file:///m:/Documents/ai-os-memory/llm_compression/arrow_storage.py)

æ”¹è¿› [query_by_similarity()](file:///m:/Documents/ai-os-memory/llm_compression/arrow_storage.py#568-636) æ–¹æ³•ï¼Œæ”¯æŒ**å‘é‡åŒ–æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—**ï¼Œæ›¿ä»£å½“å‰é€è¡Œ Python å¾ªç¯ï¼š

```diff
 def query_by_similarity(self, category, query_embedding, top_k=10, threshold=0.0):
-    # å½“å‰: é€è¡Œ Python å¾ªç¯ (O(n) Python çº§åˆ«)
-    for i in range(len(table)):
-        row = table.slice(i, 1)
-        embedding = np.array(row['embedding'][0].as_py(), dtype=np.float32)
-        similarity = np.dot(query_vec, embedding) / (query_norm * embedding_norm)

+    # æ”¹è¿›: Arrow â†’ NumPy æ‰¹é‡è®¡ç®— (é›¶æ‹·è´ + SIMD å‘é‡åŒ–)
+    embeddings_column = table.column('embedding')
+    embeddings_matrix = np.array([e.as_py() for e in embeddings_column], dtype=np.float32)
+    similarities = embeddings_matrix @ query_vec / (norms * query_norm)
```

---

### Phase 3: æ„å»ºç¼ºå¤±çš„è¯­ä¹‰ç´¢å¼•æ¨¡å—

åŸºäº ArrowEngine + ArrowStorage å®ç°åŸ [tasks.md](file:///m:/Documents/ai-os-memory/.kiro/specs/phase-2-quality-optimization/tasks.md) ä¸­ 5 ä¸ªæ ‡è®°ä¸ºå®Œæˆä½†å®é™…ç¼ºå¤±çš„æ¨¡å—ï¼š

#### [NEW] [llm_compression/vector_search.py](file:///m:/Documents/ai-os-memory/llm_compression/vector_search.py)

**æ ¸å¿ƒç±»**: `VectorSearch` â€” åŸºäº ArrowEngine çš„å‘é‡æ£€ç´¢å¼•æ“

- [search(query: str, top_k: int) -> List[SearchResult]](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#258-299) â€” è¯­ä¹‰æœç´¢
- `index(memories: List[CompressedMemory])` â€” æ‰¹é‡ç´¢å¼•
- åº•å±‚è°ƒç”¨ `EmbeddingProvider.encode()` + `ArrowStorage.query_by_similarity()`

#### [NEW] [llm_compression/semantic_indexer.py](file:///m:/Documents/ai-os-memory/llm_compression/semantic_indexer.py)

**æ ¸å¿ƒç±»**: `SemanticIndexer` â€” è¯­ä¹‰ç´¢å¼•æ„å»ºå™¨

- `index_memory(memory: CompressedMemory)` â€” ç´¢å¼•å•æ¡è®°å¿†
- `rebuild_index(category: str)` â€” é‡å»ºæŒ‡å®šåˆ†ç±»çš„å…¨é‡ç´¢å¼•
- é›†æˆ [ArrowEngine](file:///m:/Documents/ai-os-memory/llm_compression/inference/arrow_engine.py#31-354) ç”ŸæˆåµŒå…¥ + [ArrowStorage](file:///m:/Documents/ai-os-memory/llm_compression/arrow_storage.py#235-961) æŒä¹…åŒ–

#### [NEW] [llm_compression/semantic_index_db.py](file:///m:/Documents/ai-os-memory/llm_compression/semantic_index_db.py)

**æ ¸å¿ƒç±»**: `SemanticIndexDB` â€” è¯­ä¹‰ç´¢å¼•æ•°æ®åº“

- Arrow/Parquet å­˜å‚¨çš„åµŒå…¥ç´¢å¼•
- æ”¯æŒå¢é‡æ›´æ–°å’Œæ‰¹é‡é‡å»º
- ä¸ ArrowStorage å…±äº«å­˜å‚¨è·¯å¾„

#### [NEW] [llm_compression/memory_search.py](file:///m:/Documents/ai-os-memory/llm_compression/memory_search.py)

**æ ¸å¿ƒç±»**: `MemorySearch` â€” ç»Ÿä¸€è®°å¿†æ£€ç´¢æ¥å£

- [search(query: str, mode: SearchMode)](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#258-299) â€” æ”¯æŒè¯­ä¹‰ / å®ä½“ / æ—¶é—´ / æ··åˆæœç´¢
- æ•´åˆ `VectorSearch` + `ArrowStorage.query_by_entity` + `ArrowStorage.query_by_time_range`

#### [NEW] [llm_compression/background_queue.py](file:///m:/Documents/ai-os-memory/llm_compression/background_queue.py)

**æ ¸å¿ƒç±»**: `BackgroundQueue` â€” å¼‚æ­¥åå°å¤„ç†é˜Ÿåˆ—

- å¼‚æ­¥ç´¢å¼•æ›´æ–°ï¼ˆæ–°è®°å¿†å­˜å…¥åè‡ªåŠ¨è§¦å‘åµŒå…¥è®¡ç®—å’Œç´¢å¼•æ›´æ–°ï¼‰
- `asyncio` ä»»åŠ¡é˜Ÿåˆ— + æ‰¹é‡å¤„ç†ä¼˜åŒ–
- ä¸ `SemanticIndexer` é›†æˆ

#### [MODIFY] [.kiro/specs/phase-2-quality-optimization/tasks.md](file:///m:/Documents/ai-os-memory/.kiro/specs/phase-2-quality-optimization/tasks.md)

æ›´æ­£ Task 4, 7, 8, 9, 10 çš„çŠ¶æ€ä» `[x]` æ”¹ä¸º `[ ]`ã€‚

---

### Phase 4: æ—§åµŒå…¥ä½“ç³»è¿ç§»ä¸æ·˜æ±°

#### [MODIFY] 6 ä¸ªä¸‹æ¸¸æ¨¡å—

å°†ä»¥ä¸‹æ¨¡å—ä¸­çš„ [LocalEmbedder](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#31-306) / [LocalEmbedderArrow](file:///m:/Documents/ai-os-memory/llm_compression/embedder_arrow.py#25-433) å¼•ç”¨æ›¿æ¢ä¸º `EmbeddingProvider`ï¼š

| æ¨¡å— | å½“å‰ä¾èµ– | æ”¹ä¸º |
|------|---------|------|
| [cognitive_loop_arrow.py](file:///m:/Documents/ai-os-memory/llm_compression/cognitive_loop_arrow.py) | [LocalEmbedderArrow](file:///m:/Documents/ai-os-memory/llm_compression/embedder_arrow.py#25-433) | `EmbeddingProvider` |
| [batch_processor_arrow.py](file:///m:/Documents/ai-os-memory/llm_compression/batch_processor_arrow.py) | [LocalEmbedderArrow](file:///m:/Documents/ai-os-memory/llm_compression/embedder_arrow.py#25-433) | `EmbeddingProvider` |
| [embedder_adaptive.py](file:///m:/Documents/ai-os-memory/llm_compression/embedder_adaptive.py) | [LocalEmbedder](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#31-306) + [LocalEmbedderArrow](file:///m:/Documents/ai-os-memory/llm_compression/embedder_arrow.py#25-433) | `EmbeddingProvider` |
| [stored_memory.py](file:///m:/Documents/ai-os-memory/llm_compression/stored_memory.py) | [LocalEmbedder](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#31-306) | `EmbeddingProvider` |
| [batch_optimizer.py](file:///m:/Documents/ai-os-memory/llm_compression/batch_optimizer.py) | [LocalEmbedder](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#31-306) (docstring) | `EmbeddingProvider` |
| [__init__.py](file:///m:/Documents/ai-os-memory/llm_compression/__init__.py) | ç›´æ¥å¯¼å‡º [LocalEmbedder](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py#31-306) | å¯¼å‡º `EmbeddingProvider` |

#### [MODIFY] æ—§ embedder æ–‡ä»¶æ·»åŠ  deprecation è­¦å‘Š

ä¸º [embedder.py](file:///m:/Documents/ai-os-memory/llm_compression/embedder.py), [embedder_arrow.py](file:///m:/Documents/ai-os-memory/tests/unit/test_embedder_arrow.py), [embedder_adaptive.py](file:///m:/Documents/ai-os-memory/llm_compression/embedder_adaptive.py), [embedder_cache.py](file:///m:/Documents/ai-os-memory/llm_compression/embedder_cache.py) æ·»åŠ  `warnings.warn("Deprecated, use EmbeddingProvider", DeprecationWarning)`ã€‚æš‚ä¸åˆ é™¤ï¼Œä¿æŒå‘åå…¼å®¹ã€‚

---

### Phase 5: ç”Ÿäº§å°±ç»ª

#### [NEW] [Dockerfile](file:///m:/Documents/ai-os-memory/Dockerfile)

åŸºäº ArrowEngine çš„ Docker é•œåƒï¼ˆPython 3.11 slim + PyTorch CPU + Arrow + Rust tokenizersï¼‰

#### [NEW] [docker-compose.yml](file:///m:/Documents/ai-os-memory/docker-compose.yml)

å•å‘½ä»¤å¯åŠ¨ ArrowEngine æœåŠ¡

#### [MODIFY] æ–‡æ¡£æ•´ç†

- åˆå¹¶ 20+ æ ¹ç›®å½•è¿›åº¦æŠ¥å‘Šä¸º `CHANGELOG.md`
- æ›´æ–° [README.md](file:///m:/Documents/ai-os-memory/README.md) ä»¥ ArrowEngine ä¸ºæ ¸å¿ƒå™è¿°

---

## å®æ–½ä¼˜å…ˆçº§ä¸æ—¶é—´çº¿

```mermaid
gantt
    title ArrowEngine æ ¸å¿ƒè·¯å¾„å®æ–½è®¡åˆ’
    dateFormat YYYY-MM-DD
    axisFormat %m/%d
    
    section Phase 0: InferenceCore
    å®Œæ•´ Transformer å®ç°     :crit, p0_1, 2026-02-19, 3d
    å•å…ƒæµ‹è¯•è¡¥å……              :p0_2, after p0_1, 1d
    ç²¾åº¦å¯¹æ¯”éªŒè¯              :p0_3, after p0_2, 1d
    
    section Phase 1: ç«¯åˆ°ç«¯éªŒè¯
    å…¨é“¾è·¯è·‘é€š                :p1_1, after p0_3, 1d
    æ€§èƒ½åŸºå‡†æµ‹è¯•              :p1_2, after p1_1, 1d
    
    section Phase 2: ç»Ÿä¸€æ•°æ®è·¯å¾„
    EmbeddingProvider æ¥å£    :p2_1, after p1_2, 1d
    ArrowStorage é›†æˆ         :p2_2, after p2_1, 1d
    
    section Phase 3: è¯­ä¹‰ç´¢å¼•æ¨¡å—
    5 ä¸ªç¼ºå¤±æ¨¡å—å®ç°          :p3_1, after p2_2, 3d
    
    section Phase 4: è¿ç§»
    ä¸‹æ¸¸æ¨¡å—è¿ç§»              :p4_1, after p3_1, 2d
    
    section Phase 5: ç”Ÿäº§å°±ç»ª
    Docker + CI/CD            :p5_1, after p4_1, 2d
```

| Phase | é¢„ä¼°å·¥æ—¶ | é£é™©ç­‰çº§ | æˆåŠŸæ ‡å‡† |
|-------|---------|---------|---------|
| Phase 0 | 14-18h | ğŸ”´ é«˜ | InferenceCore ç²¾åº¦ â‰¥0.99ï¼ˆvs STï¼‰ |
| Phase 1 | 4-6h | ğŸŸ¡ ä¸­ | å…¨é“¾è·¯è·‘é€š + æ€§èƒ½è¾¾æ ‡ |
| Phase 2 | 4-6h | ğŸŸ¢ ä½ | EmbeddingProvider æ¥å£å¯ç”¨ |
| Phase 3 | 6-8h | ğŸŸ¡ ä¸­ | 5 ä¸ªæ¨¡å—å®ç°ä¸”æµ‹è¯•é€šè¿‡ |
| Phase 4 | 3-4h | ğŸŸ¢ ä½ | æ‰€æœ‰ä¸‹æ¸¸æ¨¡å—è¿ç§»å®Œæˆ |
| Phase 5 | 3-4h | ğŸŸ¢ ä½ | Docker ä¸€é”®å¯åŠ¨ |
| **åˆè®¡** | **32-45h** | | |

---

## Verification Plan

### Automated Tests

**Phase 0 éªŒè¯ â€” InferenceCore ç²¾åº¦**:

```bash
# ç°æœ‰æµ‹è¯• (ç¡®ä¿ä¸å¼•å…¥å›å½’)
cd m:\Documents\ai-os-memory
python -m pytest tests/unit/tools/test_model_converter.py -v

# æ–°å¢æµ‹è¯•
python -m pytest tests/unit/inference/test_inference_core.py -v
python -m pytest tests/unit/inference/test_weight_loader.py -v
python -m pytest tests/unit/inference/test_arrow_engine.py -v
```

**Phase 1 éªŒè¯ â€” ç«¯åˆ°ç«¯ç²¾åº¦å¯¹æ¯”**:

```bash
# ç«¯åˆ°ç«¯ç²¾åº¦éªŒè¯ (éœ€è¦å…ˆè½¬æ¢æ¨¡å‹)
python -m llm_compression.tools.cli convert \
    --model sentence-transformers/all-MiniLM-L6-v2 \
    --output ./models/minilm --float16 --validate

# ç²¾åº¦å¯¹æ¯”æµ‹è¯•
python -m pytest tests/integration/inference/test_e2e_precision.py -v

# æ€§èƒ½åŸºå‡†
python benchmarks/arrowengine_benchmark.py
```

**Phase 2 éªŒè¯ â€” ç»Ÿä¸€æ•°æ®è·¯å¾„**:

```bash
# ArrowStorage é›†æˆæµ‹è¯•
python -m pytest tests/integration/arrow/test_arrow_integration.py -v

# ç°æœ‰ API æµ‹è¯• (ç¡®ä¿æ— å›å½’)
python -m pytest tests/integration/server/test_api.py -v
```

**Phase 3 éªŒè¯ â€” è¯­ä¹‰ç´¢å¼•æ¨¡å—**:

```bash
# å„æ¨¡å—å•å…ƒæµ‹è¯•
python -m pytest tests/unit/test_vector_search.py -v
python -m pytest tests/unit/test_semantic_indexer.py -v
python -m pytest tests/unit/test_memory_search.py -v
python -m pytest tests/unit/test_background_queue.py -v
```

**Phase 4 éªŒè¯ â€” è¿ç§»åå›å½’æµ‹è¯•**:

```bash
# å…¨é‡å›å½’æµ‹è¯•
python -m pytest tests/ -v --ignore=tests/load --ignore=tests/performance

# ç°æœ‰éªŒè¯è„šæœ¬
python verify_arrowengine.py
```

### Manual Verification

**Phase 1 æ‰‹åŠ¨éªŒè¯** â€” éœ€è¦ç”¨æˆ·å‚ä¸ï¼š
1. è¿è¡Œ `python -m llm_compression.tools.cli convert --model sentence-transformers/all-MiniLM-L6-v2 --output ./models/minilm --float16` ç¡®è®¤æ¨¡å‹è½¬æ¢æˆåŠŸ
2. è¿è¡Œ `python verify_arrowengine.py` ç¡®è®¤æ‰€æœ‰æ­¥éª¤ âœ… é€šè¿‡
3. æ£€æŸ¥ `benchmarks/arrowengine_benchmark.py` è¾“å‡ºçš„æ€§èƒ½æ•°æ®æ˜¯å¦è¾¾æ ‡

> [!NOTE]
> æ­¥éª¤ 1 éœ€è¦ç½‘ç»œè®¿é—®æ¥ä¸‹è½½ HuggingFace æ¨¡å‹ï¼ˆçº¦ 80MBï¼‰ï¼Œé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿã€‚

---

## é£é™©ä¸ç¼“è§£

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|---------|
| InferenceCore ç²¾åº¦ä¸è¾¾æ ‡ | Phase 0-1 é˜»å¡ | ä¿ç•™ `SentenceTransformerProvider` åå¤‡ï¼›é€å±‚å¯¹æ¯”æƒé‡åŠ è½½ç»“æœå®šä½ç²¾åº¦æŸå¤±æ¥æº |
| æ¨¡å‹æ ¼å¼å…¼å®¹æ€§ | Phase 0 é˜»å¡ | [ModelConverter](file:///m:/Documents/ai-os-memory/llm_compression/tools/model_converter.py#81-533) å·²æœ‰éªŒè¯é€»è¾‘ï¼›è½¬æ¢åç«‹å³å¯¹æ¯”æƒé‡æ•°å€¼ |
| ä¸‹æ¸¸æ¨¡å—è¿ç§»å¼•å…¥ bug | Phase 4 å›å½’ | Protocol æ¥å£ + å……åˆ†å•å…ƒæµ‹è¯•ï¼›æ¸è¿›å¼è¿ç§»ï¼Œæ¯ä¸ªæ¨¡å—ç‹¬ç«‹ PR |
| Float16 ç²¾åº¦æŸå¤± | Phase 2 å½±å“ | ArrowStorage å·²ä½¿ç”¨ float16ï¼Œå·²è¢«æ¥å—ï¼›å¤§ embedding ç»´åº¦ä¸‹å½±å“ <0.1% |
