# ArrowEngine Quick Start Guide

Get started with ArrowEngine in 5 minutes! This guide covers installation, model conversion, basic usage, and Docker deployment.

## What is ArrowEngine?

ArrowEngine is a high-performance embedding inference engine that replaces sentence-transformers in the AI-OS memory system. It delivers:

- **21.4x faster model loading** (< 100ms vs 2-5s)
- **2-4x faster inference** (< 5ms vs 10-20ms per sequence)
- **50% memory reduction** (float16 weights + zero-copy data flow)
- **Perfect precision** (â‰¥ 0.999999 cosine similarity vs sentence-transformers)
- **Zero API costs** (self-hosted inference)

## Prerequisites

- Python 3.11+
- 2GB RAM minimum
- 500MB disk space for model

## Installation

### 1. Clone and Setup

```bash
# Clone repository
git clone https://github.com/ai-os/llm-compression.git
cd llm-compression

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Install package in editable mode
pip install -e .
```

### 2. Verify Installation

```bash
python -c "from llm_compression.embedding_provider import get_default_provider; print('âœ… Installation successful!')"
```

## Model Conversion

Convert the sentence-transformers model to ArrowEngine format:

```bash
python scripts/convert_and_validate.py
```

This script will:
1. Download `all-MiniLM-L6-v2` from HuggingFace
2. Convert to Arrow/Parquet format with float16 optimization
3. Extract fast Rust tokenizer
4. Run precision validation tests
5. Run performance benchmarks

**Output:**
```
ArrowEngine Conversion and Validation
============================================================
Step 1: Converting Model
============================================================
Converting sentence-transformers/all-MiniLM-L6-v2 to ./models/minilm...
âœ… Conversion successful!
   Output: ./models/minilm
   Original size: 90.9 MB
   Compressed size: 45.2 MB
   Compression ratio: 2.01x

Step 2: Running Precision Validation
============================================================
âœ… Precision tests passed!

Step 3: Running Performance Benchmark
============================================================
âœ… Performance benchmarks passed!

============================================================
âœ… Validation Complete!
============================================================
```

**Model Structure:**
```
models/minilm/
â”œâ”€â”€ weights.parquet      # Model weights (Arrow format)
â”œâ”€â”€ tokenizer.json       # Fast Rust tokenizer
â””â”€â”€ metadata.json        # Model configuration
```

## Basic Usage

### Simple Encoding

```python
from llm_compression.embedding_provider import get_default_provider

# Initialize provider (auto-selects ArrowEngine or falls back to sentence-transformers)
provider = get_default_provider()

# Encode single text
embedding = provider.encode("Hello, world!")
print(f"Embedding shape: {embedding.shape}")  # (384,)

# Encode multiple texts
texts = ["Machine learning", "Deep learning", "Neural networks"]
embeddings = provider.encode_batch(texts)
print(f"Batch shape: {embeddings.shape}")  # (3, 384)

# Compute similarity
sim = provider.similarity(embeddings[0], embeddings[1])
print(f"Similarity: {sim:.3f}")  # 0.847
```

### Semantic Search

```python
from llm_compression.embedding_provider import get_default_provider
from llm_compression.arrow_storage import ArrowStorage
from llm_compression.vector_search import VectorSearch
from llm_compression.semantic_index_db import SemanticIndexDB

# Setup components
provider = get_default_provider()
storage = ArrowStorage("./data/memories.parquet")
index_db = SemanticIndexDB("./data/index")

# Create search engine
search = VectorSearch(provider, storage, index_db)

# Search for similar memories
results = search.search(
    query="machine learning concepts",
    category="knowledge",
    top_k=5,
    threshold=0.7
)

for result in results:
    print(f"{result.memory_id}: {result.similarity:.3f}")
    print(f"Content: {result.memory['content']}\n")
```

### Memory Indexing

```python
from llm_compression.semantic_indexer import SemanticIndexer

# Create indexer
indexer = SemanticIndexer(provider, storage, index_db)

# Index a single memory
memory = {
    'memory_id': 'mem_1',
    'category': 'knowledge',
    'context': 'Machine learning is a subset of artificial intelligence.',
    'timestamp': datetime.now(),
    'embedding': None  # Will be generated automatically
}
indexer.index_memory(memory)

# Batch index multiple memories
memories = [
    {'memory_id': 'mem_2', 'category': 'knowledge', 'context': 'Deep learning uses neural networks.'},
    {'memory_id': 'mem_3', 'category': 'code', 'context': 'def hello(): print("Hello")'},
]
indexer.batch_index(memories, batch_size=32)

# Rebuild entire index
indexer.rebuild_index(category="knowledge")
```

### Background Indexing

```python
import asyncio
from llm_compression.background_queue import BackgroundQueue

async def main():
    # Create background queue
    queue = BackgroundQueue(indexer, batch_size=32)
    
    # Start background worker
    await queue.start()
    
    # Submit memories for async indexing (non-blocking)
    memories = [...]  # Your memories
    await queue.submit_batch(memories)
    
    # Continue other work while indexing happens in background
    print("Indexing in progress...")
    
    # Wait for completion (optional)
    await queue.wait_until_empty(timeout=60.0)
    
    # Stop queue
    await queue.stop()
    print("All memories indexed!")

asyncio.run(main())
```

## Docker Deployment

### Quick Start with Docker Compose

```bash
# Build and start service
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f arrowengine

# Stop service
docker-compose down
```

### Manual Docker Build

```bash
# Build image
docker build -t arrowengine:latest .

# Run container
docker run -d \
  --name arrowengine \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models:ro \
  -v $(pwd)/logs:/app/logs \
  -e MODEL_PATH=/app/models/minilm \
  -e DEVICE=cpu \
  -e PORT=8000 \
  arrowengine:latest

# Check health
curl http://localhost:8000/health

# View logs
docker logs -f arrowengine

# Stop container
docker stop arrowengine
docker rm arrowengine
```

### Environment Variables

Configure the container using environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_PATH` | `/app/models/minilm` | Path to converted model |
| `DEVICE` | `cpu` | Device for inference (`cpu`, `cuda`, `mps`) |
| `PORT` | `8000` | API server port |
| `API_KEY` | - | Optional API key for authentication |
| `LOG_LEVEL` | `INFO` | Logging level |
| `LOG_FORMAT` | `json` | Log format (`json` or `text`) |
| `WORKERS` | `4` | Number of worker processes |
| `MAX_BATCH_SIZE` | `32` | Maximum batch size |

### Docker Compose Configuration

Edit `docker-compose.yml` to customize:

```yaml
services:
  arrowengine:
    environment:
      # Model configuration
      - MODEL_PATH=/app/models/minilm
      - DEVICE=cpu
      
      # API configuration
      - PORT=8000
      - API_KEY=${API_KEY:-}
      
      # Performance tuning
      - WORKERS=4
      - MAX_BATCH_SIZE=32
    
    # Resource limits
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 2G
```

### Health Check

The container includes a health check endpoint:

```bash
# Check health
curl http://localhost:8000/health

# Response
{
  "status": "healthy",
  "model": "all-MiniLM-L6-v2",
  "device": "cpu",
  "embedding_dimension": 384
}
```

### API Endpoints

Once deployed, the service exposes these endpoints:

```bash
# Encode single text
curl -X POST http://localhost:8000/encode \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello, world!"}'

# Encode batch
curl -X POST http://localhost:8000/encode_batch \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Text 1", "Text 2", "Text 3"]}'

# Compute similarity
curl -X POST http://localhost:8000/similarity \
  -H "Content-Type: application/json" \
  -d '{"text1": "Machine learning", "text2": "Deep learning"}'

# Metrics (Prometheus format)
curl http://localhost:8000/metrics
```

## Performance Comparison

### Model Loading

```python
import time

# ArrowEngine
start = time.time()
provider = ArrowEngineProvider(model_path="./models/minilm")
arrow_time = (time.time() - start) * 1000
print(f"ArrowEngine: {arrow_time:.1f}ms")  # ~50ms

# sentence-transformers
start = time.time()
model = SentenceTransformer("all-MiniLM-L6-v2")
st_time = (time.time() - start) * 1000
print(f"sentence-transformers: {st_time:.1f}ms")  # ~2500ms

print(f"Speedup: {st_time/arrow_time:.1f}x")  # ~50x
```

### Inference Latency

```python
import numpy as np

text = "Machine learning is a subset of artificial intelligence."

# ArrowEngine
latencies = []
for _ in range(100):
    start = time.time()
    provider.encode(text)
    latencies.append((time.time() - start) * 1000)
arrow_latency = np.median(latencies)
print(f"ArrowEngine: {arrow_latency:.2f}ms")  # ~3ms

# sentence-transformers
latencies = []
for _ in range(100):
    start = time.time()
    model.encode(text)
    latencies.append((time.time() - start) * 1000)
st_latency = np.median(latencies)
print(f"sentence-transformers: {st_latency:.2f}ms")  # ~12ms

print(f"Speedup: {st_latency/arrow_latency:.1f}x")  # ~4x
```

### Memory Usage

```python
import psutil
import os

process = psutil.Process(os.getpid())

# ArrowEngine
provider = ArrowEngineProvider(model_path="./models/minilm")
arrow_mem = process.memory_info().rss / (1024 * 1024)
print(f"ArrowEngine: {arrow_mem:.1f}MB")  # ~95MB

# sentence-transformers (in separate process)
# model = SentenceTransformer("all-MiniLM-L6-v2")
# st_mem = process.memory_info().rss / (1024 * 1024)
# print(f"sentence-transformers: {st_mem:.1f}MB")  # ~180MB

print(f"Memory reduction: {(1 - arrow_mem/180)*100:.1f}%")  # ~47%
```

## Troubleshooting

### Model Not Found

**Problem:**
```
FileNotFoundError: Model path not found: ./models/minilm
```

**Solution:**
Run the conversion script:
```bash
python scripts/convert_and_validate.py
```

### Import Errors

**Problem:**
```
ImportError: cannot import name 'get_default_provider'
```

**Solution:**
Reinstall the package:
```bash
pip install -e . --upgrade
```

### Slow Performance

**Problem:**
ArrowEngine is slower than expected.

**Solution:**
1. Verify model is converted:
   ```bash
   ls -lh models/minilm/
   ```

2. Check provider type:
   ```python
   provider = get_default_provider()
   print(type(provider))  # Should be ArrowEngineProvider
   ```

3. Use batch operations:
   ```python
   # Good: Batch encoding
   embeddings = provider.encode_batch(texts, batch_size=32)
   
   # Bad: Loop encoding
   embeddings = [provider.encode(text) for text in texts]
   ```

### Docker Container Won't Start

**Problem:**
Container exits immediately.

**Solution:**
1. Check logs:
   ```bash
   docker logs arrowengine
   ```

2. Verify model is mounted:
   ```bash
   docker exec arrowengine ls -lh /app/models/minilm/
   ```

3. Check environment variables:
   ```bash
   docker exec arrowengine env | grep MODEL
   ```

### Low Precision

**Problem:**
Embeddings differ from sentence-transformers.

**Solution:**
This is expected due to float16 optimization. The difference is minimal (similarity â‰¥ 0.999999). For exact reproducibility, use:
```python
from llm_compression.embedding_provider import SentenceTransformerProvider
provider = SentenceTransformerProvider()
```

## Next Steps

### Learn More

- **[API Reference](API_REFERENCE.md)** - Complete API documentation
- **[Migration Guide](MIGRATION_GUIDE.md)** - Migrate from sentence-transformers
- **[Performance Tuning](PERFORMANCE_TUNING.md)** - Optimize for your use case

### Advanced Topics

- **Custom Models** - Convert your own models
- **GPU Acceleration** - Use CUDA for faster inference
- **Distributed Deployment** - Scale across multiple nodes
- **Monitoring** - Set up Prometheus metrics

### Examples

Check the `examples/` directory for more usage patterns:

```bash
# Basic usage
python examples/quick_start.py

# Semantic search
python examples/vector_search_example.py

# Background indexing
python examples/background_queue_example.py

# Chat agent with ArrowEngine
python examples/test_arrowengine_chat.py
```

## Summary

You've learned how to:

- âœ… Install ArrowEngine
- âœ… Convert models to Arrow format
- âœ… Encode text to embeddings
- âœ… Perform semantic search
- âœ… Index memories
- âœ… Deploy with Docker

**Key Takeaways:**

1. **Use `get_default_provider()`** for automatic provider selection
2. **Convert models once** with `convert_and_validate.py`
3. **Use batch operations** for best performance
4. **Deploy with Docker** for production
5. **Monitor with Prometheus** for observability

## Getting Help

- **Documentation**: See `docs/` directory
- **Examples**: See `examples/` directory
- **Issues**: Report bugs on GitHub
- **Questions**: Ask in discussions

---

**Ready to build?** Start with the [API Reference](API_REFERENCE.md) or check out the [examples](../examples/).

**Need to migrate?** Follow the [Migration Guide](MIGRATION_GUIDE.md).

**Want to optimize?** Read the [Performance Tuning Guide](PERFORMANCE_TUNING.md).

Happy coding! ðŸš€
