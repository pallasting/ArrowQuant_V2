# Phase 2.0 å¿«é€Ÿå¼€å§‹æŒ‡å—

**ç‰ˆæœ¬**: 2.0  
**æœ€åæ›´æ–°**: 2026-02-17

---

## ç›®å½•

1. [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
2. [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
3. [å¿«é€ŸéªŒè¯](#å¿«é€ŸéªŒè¯)
4. [åŸºç¡€ä½¿ç”¨](#åŸºç¡€ä½¿ç”¨)
5. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç¯å¢ƒè¦æ±‚

### ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windows / Linux / macOS
- **Python**: 3.10+
- **å†…å­˜**: 4GB+ (æ¨è 8GB+)
- **å­˜å‚¨**: 2GB+ å¯ç”¨ç©ºé—´

### ä¾èµ–æœåŠ¡

- **Ollama** (å¯é€‰): æœ¬åœ° LLM æœåŠ¡
  - ä¸‹è½½: https://ollama.ai/
  - é»˜è®¤ç«¯å£: 11434

---

## å®‰è£…æ­¥éª¤

### 1. å…‹éš†ä»“åº“

```bash
git clone https://github.com/your-org/ai-os-memory.git
cd ai-os-memory
```

### 2. å®‰è£…ä¾èµ–

```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install -r requirements.txt

# å®‰è£…ä¸ºå¯ç¼–è¾‘åŒ…ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
pip install -e .
```

### 3. éªŒè¯å®‰è£…

```bash
# è¿è¡Œæµ‹è¯•
pytest tests/unit/ -v

# æŸ¥çœ‹ç‰ˆæœ¬
python -c "import llm_compression; print(llm_compression.__version__)"
```

---

## å¿«é€ŸéªŒè¯

### è¿è¡ŒåŠŸèƒ½æµ‹è¯•

```bash
python examples/test_chat_agent.py
```

**é¢„æœŸè¾“å‡º**:
```
==============================================================
Phase 2.0 åŠŸèƒ½æµ‹è¯•
==============================================================

1ï¸âƒ£  æµ‹è¯•æ¨¡å‹ç¼“å­˜...
   âœ“ æ¨¡å‹é¢„åŠ è½½å®Œæˆ: 25.43s

2ï¸âƒ£  åˆå§‹åŒ–ç»„ä»¶...
   âœ“ æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ

3ï¸âƒ£  æµ‹è¯•è®°å¿†æ·»åŠ ...
   âœ“ æ·»åŠ  5 æ¡è®°å¿†: 0.52s
   âœ“ å¹³å‡é€Ÿåº¦: 9.6 memories/s

4ï¸âƒ£  æµ‹è¯•å¯¹è¯åŠŸèƒ½...
   Q: ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ
   A: Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...
   âœ“ å“åº”æ—¶é—´: 1.23s
   âœ“ ä½¿ç”¨è®°å¿†: 3
   âœ“ è´¨é‡åˆ†æ•°: 0.85

5ï¸âƒ£  æµ‹è¯•ä¼˜åŒ–ç»Ÿè®¡...
   âœ“ ä¼˜åŒ–å·²å¯ç”¨
   âœ“ ç¼“å­˜æ¨¡å‹æ•°: 1
   âœ“ è‡ªé€‚åº”è°ƒç”¨: 5
   âœ“ æ‰¹é‡å¤„ç†: 5 items

6ï¸âƒ£  æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯...
   âœ“ å¯¹è¯è½®æ¬¡: 3
   âœ“ è®°å¿†æ•°é‡: 5
   âœ“ è¿æ¥æ•°é‡: 10

==============================================================
âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼
==============================================================
```

---

## åŸºç¡€ä½¿ç”¨

### 1. å¯åŠ¨äº¤äº’å¼å¯¹è¯

```bash
python examples/chat_agent_optimized.py
```

### 2. åŸºç¡€å¯¹è¯

```
ğŸ’¬ You: ä½ å¥½ï¼

ğŸ¤– Agent: ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªå…·æœ‰æŒç»­å­¦ä¹ èƒ½åŠ›çš„å¯¹è¯åŠ©æ‰‹ã€‚
   ğŸ“Š Quality: 0.85 | Memories: 0 | Learning: âœ… | Time: 0.52s

ğŸ’¬ You: ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ

ğŸ¤– Agent: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ...
   ğŸ“Š Quality: 0.92 | Memories: 2 | Learning: âœ… | Time: 1.15s
```

### 3. ä½¿ç”¨å‘½ä»¤

```
ğŸ’¬ You: /help

ğŸ“– Available Commands:
  /help              - Show this help message
  /stats             - Show conversation statistics
  /optimization      - Show optimization statistics
  /benchmark         - Run performance benchmark
  /import <file>     - Import file as memories
  /clear             - Clear conversation history
  /quit, /exit       - Exit the chat
```

---

## é«˜çº§åŠŸèƒ½

### 1. å¯¼å…¥æ–‡ä»¶ä½œä¸ºè®°å¿†

```
ğŸ’¬ You: /import docs/README.md

ğŸ“¥ Importing: docs/README.md
  ğŸ“„ Found 15 chunks
  âœ… Imported 15 new memories
  âœ… Total memories: 20
```

### 2. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯

```
ğŸ’¬ You: /stats

ğŸ“Š Conversation Statistics:
  â€¢ Total turns: 10
  â€¢ Memory count: 20
  â€¢ Connections: 45
  â€¢ Avg connections: 2.25
  â€¢ Session time: 125.3s

  User Profile:
    â€¢ Total interactions: 10
    â€¢ Top interests:
      - machine learning: 0.85
      - python: 0.72
      - ai: 0.68
```

### 3. æŸ¥çœ‹ä¼˜åŒ–ç»Ÿè®¡

```
ğŸ’¬ You: /optimization

âš¡ Optimization Statistics:

  Model Cache:
    â€¢ Cached models: 1
    â€¢ Models: all-MiniLM-L6-v2

  Adaptive Embedder:
    â€¢ Total calls: 25
    â€¢ Traditional: 15 (60.0%)
    â€¢ Arrow: 10 (40.0%)
    â€¢ Total items: 250

  Batch Processor:
    â€¢ Items processed: 250
    â€¢ Batches: 5
    â€¢ Avg throughput: 1285.3 items/s
    â€¢ Batch size: 50
    â€¢ Workers: 4
```

### 4. è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•

```
ğŸ’¬ You: /benchmark

ğŸƒ Running Performance Benchmark...
  This will test memory operations performance

  Test 1: Batch Memory Addition
    âœ“ Added 100 memories in 0.78s
    âœ“ Throughput: 128.2 memories/s

  Test 2: Memory Retrieval
    âœ“ 10 retrievals in 0.35s
    âœ“ Avg retrieval time: 35.2ms

  âœ… Benchmark Complete!
```

---

## ç¼–ç¨‹ä½¿ç”¨

### åŸºç¡€ç¤ºä¾‹

```python
import asyncio
from llm_compression import (
    LLMClient,
    LLMCompressor,
    ModelSelector,
    ConversationalAgent,
    CognitiveLoop
)
from llm_compression.embedder_cache import preload_default_model

async def main():
    # 1. é¢„åŠ è½½æ¨¡å‹ï¼ˆä¼˜åŒ–ï¼‰
    preload_default_model()
    
    # 2. åˆå§‹åŒ–ç»„ä»¶
    llm_client = LLMClient(endpoint="http://localhost:11434")
    model_selector = ModelSelector()
    compressor = LLMCompressor(llm_client, model_selector)
    
    # 3. åˆ›å»ºè®¤çŸ¥å¾ªç¯
    from llm_compression.expression_layer import MultiModalExpressor
    from llm_compression.internal_feedback import InternalFeedbackSystem
    from llm_compression.connection_learner import ConnectionLearner
    from llm_compression.network_navigator import NetworkNavigator
    from llm_compression.reconstructor import LLMReconstructor
    
    reconstructor = LLMReconstructor(llm_client=llm_client)
    expressor = MultiModalExpressor(llm_client, reconstructor)
    feedback = InternalFeedbackSystem()
    learner = ConnectionLearner()
    navigator = NetworkNavigator()
    
    cognitive_loop = CognitiveLoop(
        expressor=expressor,
        feedback=feedback,
        learner=learner,
        navigator=navigator
    )
    
    # 4. åˆ›å»ºå¯¹è¯ Agent
    agent = ConversationalAgent(
        llm_client=llm_client,
        compressor=compressor,
        cognitive_loop=cognitive_loop,
        user_id="user_001"
    )
    
    # 5. å¯¹è¯
    response = await agent.chat("ä½ å¥½ï¼")
    print(f"Agent: {response.message}")
    
    response = await agent.chat("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
    print(f"Agent: {response.message}")
    
    # 6. æŸ¥çœ‹ç»Ÿè®¡
    stats = agent.get_stats()
    print(f"Total turns: {stats['total_turns']}")
    print(f"Memory count: {stats['memory_count']}")

if __name__ == "__main__":
    asyncio.run(main())
```

### ä½¿ç”¨ä¼˜åŒ–åŠŸèƒ½

```python
from llm_compression.cognitive_loop_arrow import CognitiveLoopArrow

# åˆ›å»ºä¼˜åŒ–ç‰ˆè®¤çŸ¥å¾ªç¯
cognitive_loop_arrow = CognitiveLoopArrow(
    cognitive_loop=cognitive_loop,
    enable_optimizations=True,  # å¯ç”¨æ‰€æœ‰ä¼˜åŒ–
    adaptive_threshold=1000,    # è‡ªé€‚åº”é˜ˆå€¼
    batch_size=100,             # æ‰¹æ¬¡å¤§å°
    max_workers=4               # å¹¶è¡Œçº¿ç¨‹æ•°
)

# æŸ¥çœ‹ä¼˜åŒ–ç»Ÿè®¡
opt_stats = cognitive_loop_arrow.get_optimization_stats()
print(f"Optimizations enabled: {opt_stats['optimizations_enabled']}")
```

---

## å¸¸è§é—®é¢˜

### Q1: é¦–æ¬¡è¿è¡Œå¾ˆæ…¢ï¼Ÿ

**A**: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ embedding æ¨¡å‹ï¼ˆ~500MBï¼‰ï¼Œå¤§çº¦éœ€è¦ 30-60 ç§’ã€‚åç»­è¿è¡Œä¼šä½¿ç”¨ç¼“å­˜ï¼Œé€Ÿåº¦ä¼šå¿«å¾ˆå¤šã€‚

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨åº”ç”¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹
from llm_compression.embedder_cache import preload_default_model
preload_default_model()
```

---

### Q2: Ollama è¿æ¥å¤±è´¥ï¼Ÿ

**A**: ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œã€‚

**æ£€æŸ¥æ–¹æ³•**:
```bash
# æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags

# å¯åŠ¨ Ollamaï¼ˆå¦‚æœæœªè¿è¡Œï¼‰
ollama serve
```

---

### Q3: å†…å­˜å ç”¨è¿‡é«˜ï¼Ÿ

**A**: å¯ä»¥è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œå¹¶è¡Œçº¿ç¨‹æ•°ã€‚

**ä¼˜åŒ–æ–¹æ³•**:
```python
cognitive_loop_arrow = CognitiveLoopArrow(
    enable_optimizations=True,
    batch_size=50,      # å‡å°æ‰¹æ¬¡å¤§å°
    max_workers=2       # å‡å°‘å¹¶è¡Œçº¿ç¨‹
)
```

---

### Q4: å¦‚ä½•æå‡æ€§èƒ½ï¼Ÿ

**A**: å¯ç”¨æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ã€‚

**ä¼˜åŒ–æ¸…å•**:
1. âœ… é¢„åŠ è½½æ¨¡å‹ï¼ˆå‡å°‘é¦–æ¬¡å»¶è¿Ÿï¼‰
2. âœ… ä½¿ç”¨ CognitiveLoopArrowï¼ˆArrow é›¶æ‹·è´ï¼‰
3. âœ… å¯ç”¨è‡ªé€‚åº”åˆ‡æ¢ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ–¹æ³•ï¼‰
4. âœ… å¯ç”¨æ‰¹é‡å¤„ç†ï¼ˆæå‡ååé‡ï¼‰
5. âœ… è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œçº¿ç¨‹æ•°

---

### Q5: å¦‚ä½•æŒä¹…åŒ–è®°å¿†ï¼Ÿ

**A**: ä½¿ç”¨ Arrow å­˜å‚¨ä¿å­˜è®°å¿†ã€‚

**ç¤ºä¾‹**:
```python
from llm_compression.arrow_storage_zero_copy import ArrowStorageZeroCopy

# ä¿å­˜è®°å¿†
storage = ArrowStorageZeroCopy()
storage.save(cognitive_loop_arrow.memory_table, "memories.parquet")

# åŠ è½½è®°å¿†
memory_table = storage.load("memories.parquet")
cognitive_loop_arrow.load_memories_from_table(memory_table)
```

---

## ä¸‹ä¸€æ­¥

- ğŸ“– é˜…è¯» [API å‚è€ƒæ–‡æ¡£](API_REFERENCE.md)
- ğŸ“ é˜…è¯» [æ¶æ„è®¾è®¡æ–‡æ¡£](ARCHITECTURE.md)
- ğŸ“š é˜…è¯» [ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ](USER_GUIDE.md)
- ğŸš€ æŸ¥çœ‹ [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](PHASE_2.0_OPTIMIZATION_COMPLETION_REPORT.md)

---

## è·å–å¸®åŠ©

- **æ–‡æ¡£**: `docs/` ç›®å½•
- **ç¤ºä¾‹**: `examples/` ç›®å½•
- **æµ‹è¯•**: `tests/` ç›®å½•
- **é—®é¢˜**: GitHub Issues

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2026-02-17  
**ç»´æŠ¤è€…**: AI-OS å›¢é˜Ÿ
