# ArrowEngine Intel ä¼˜åŒ–æ€»ç»“

## å®Œæˆæ—¶é—´
2026-02-18

---

## å®æ–½çš„ä¼˜åŒ–

### æ–¹æ¡ˆ A: å¿«é€Ÿä¼˜åŒ– (å·²å®Œæˆ âœ…)

**å®æ–½å†…å®¹:**

1. **Intel MKL çº¿ç¨‹ä¼˜åŒ–**
   - è®¾ç½® intra-op çº¿ç¨‹æ•° = 12 (ç‰©ç†æ ¸å¿ƒæ•°)
   - è®¾ç½® inter-op çº¿ç¨‹æ•° = 2
   - å¯ç”¨ MKL-DNN (oneDNN) ä¼˜åŒ–

2. **ç¯å¢ƒå˜é‡ä¼˜åŒ–**
   ```
   MKL_NUM_THREADS=12
   OMP_NUM_THREADS=12
   KMP_BLOCKTIME=1 (ä½å»¶è¿Ÿ)
   KMP_AFFINITY=granularity=fine,compact,1,0
   ```

3. **ä»£ç ä¿®æ”¹**
   - æ–‡ä»¶: `llm_compression/inference/arrow_engine.py`
   - æ–°å¢: `_apply_intel_optimizations()` æ–¹æ³•
   - æ–°å¢å‚æ•°: `enable_intel_optimizations=True`

---

## éªŒè¯æµ‹è¯•ç»“æœ

### æµ‹è¯•ç¯å¢ƒ
- **CPU**: Intel64 Family 6 Model 186 (12 ç‰©ç†æ ¸å¿ƒ, 16 é€»è¾‘æ ¸å¿ƒ)
- **å†…å­˜**: 15.7 GB (å¯ç”¨ 2.7 GB)
- **PyTorch**: 2.10.0+cpu
- **MKL-DNN**: âœ… å·²å¯ç”¨

### æµ‹è¯•ç»“æœ (8/8 é€šè¿‡)

| æµ‹è¯•é¡¹ | çŠ¶æ€ | æ€§èƒ½ | å¤‡æ³¨ |
|--------|------|------|------|
| ç¯å¢ƒæ£€æŸ¥ | âœ… é€šè¿‡ | - | æ‰€æœ‰ä¾èµ–æ­£å¸¸ |
| æ¨¡å‹åŠ è½½é€Ÿåº¦ | âœ… é€šè¿‡ | 2.8s (å¹³å‡) | é¦–æ¬¡ 6.4s, åç»­ 1.0s |
| æ¨ç†å»¶è¿Ÿ | âœ… é€šè¿‡ | 36.8ms (ä¸­ä½æ•°) | P95: 44ms |
| æ‰¹é‡ååé‡ | âœ… é€šè¿‡ | 35 req/s | batch=32 æœ€ä¼˜ |
| å†…å­˜å ç”¨ | âœ… é€šè¿‡ | 289 MB | æ¨¡å‹ 280MB + æ¨ç† 9MB |
| ç²¾åº¦éªŒè¯ | âœ… é€šè¿‡ | 0.999999 | å®Œç¾åŒ¹é… |
| EmbeddingProvider | âœ… é€šè¿‡ | - | æ‰€æœ‰æ¥å£æ­£å¸¸ |
| ArrowStorage é›†æˆ | âœ… é€šè¿‡ | - | åŸºç¡€åŠŸèƒ½æ­£å¸¸ |

**æ€»ä½“æˆåŠŸç‡**: 100% (8/8)

---

## æ€§èƒ½å¯¹æ¯”

### ä¼˜åŒ–å‰ vs ä¼˜åŒ–å

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| æ¨ç†å»¶è¿Ÿ (ä¸­ä½æ•°) | ~51ms | ~37ms | **1.4x** |
| æ‰¹é‡ååé‡ | ~22 req/s | ~35 req/s | **1.6x** |
| MKL-DNN çŠ¶æ€ | æœªçŸ¥ | âœ… å¯ç”¨ | - |
| çº¿ç¨‹ä¼˜åŒ– | é»˜è®¤ | 12æ ¸å¿ƒ | - |

**å®é™…æå‡**: çº¦ 1.4-1.6x æ€§èƒ½æå‡

---

## Intel CPU ç‰¹æ€§æ£€æµ‹

### å·²ç¡®è®¤æ”¯æŒçš„ç‰¹æ€§

âœ… **Intel MKL** (Math Kernel Library)
- çŠ¶æ€: å·²å¯ç”¨
- ç”¨é€”: ä¼˜åŒ– BLAS, LAPACK ç­‰æ•°å­¦è¿ç®—

âœ… **oneDNN** (åŸ MKL-DNN)
- çŠ¶æ€: å·²å¯ç”¨  
- ç”¨é€”: ä¼˜åŒ–ç¥ç»ç½‘ç»œç®—å­ (å·ç§¯ã€æ± åŒ–ã€å½’ä¸€åŒ–)

âœ… **å¤šçº¿ç¨‹ä¼˜åŒ–**
- Intra-op: 12 çº¿ç¨‹ (ç‰©ç†æ ¸å¿ƒ)
- Inter-op: 2 çº¿ç¨‹
- äº²å’Œæ€§: Fine-grained, compact

### æ½œåœ¨æ”¯æŒçš„ç‰¹æ€§ (æœªéªŒè¯)

âš ï¸ **AVX-512** (Advanced Vector Extensions)
- éœ€è¦æ£€æµ‹: CPU å‹å·æ˜¯å¦æ”¯æŒ
- é¢„æœŸæå‡: 2-4x (å¦‚æœæ”¯æŒ)

âš ï¸ **VNNI** (Vector Neural Network Instructions)
- éœ€è¦æ£€æµ‹: Intel DL Boost æ”¯æŒ
- é¢„æœŸæå‡: 2-4x INT8 æ¨ç†

âš ï¸ **BF16** (Brain Floating Point)
- éœ€è¦æ£€æµ‹: ç¬¬ 3 ä»£ Xeon æˆ–æ›´æ–°
- é¢„æœŸæå‡: 2-3x + å†…å­˜å‡åŠ

---

## ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®

### çŸ­æœŸ (1-2 å‘¨)

#### 1. Intel Extension for PyTorch (IPEX)
**é¢„æœŸæå‡**: 2-3x

**å®‰è£…:**
```bash
pip install intel-extension-for-pytorch
```

**é›†æˆ:**
```python
import intel_extension_for_pytorch as ipex

# åœ¨ ArrowEngine.__init__ ä¸­
if use_ipex and IPEX_AVAILABLE:
    self.inference_core = ipex.optimize(
        self.inference_core,
        dtype=torch.float32,  # æˆ– torch.bfloat16
        level="O1"
    )
```

**ä¼˜åŠ¿:**
- æœ€å°ä»£ç æ”¹åŠ¨
- è‡ªåŠ¨ç®—å­èåˆ
- æ”¯æŒ BF16 æ··åˆç²¾åº¦

---

### ä¸­æœŸ (2-4 å‘¨)

#### 2. OpenVINO æ¨ç†å¼•æ“
**é¢„æœŸæå‡**: 3-5x

**è½¬æ¢æµç¨‹:**
```bash
# PyTorch -> ONNX
python -m torch.onnx.export model.pt model.onnx

# ONNX -> OpenVINO IR
mo --input_model model.onnx --output_dir openvino_model
```

**ä¼˜åŠ¿:**
- æœ€å¤§æ€§èƒ½æå‡
- æ”¯æŒ INT8 é‡åŒ–
- æ”¯æŒå¤šè®¾å¤‡ (CPU/GPU/NPU)
- ç®—å­èåˆ + å›¾ä¼˜åŒ–

---

### é•¿æœŸ (1-2 æœˆ)

#### 3. Intel GPU åŠ é€Ÿ (å¦‚æœå¯ç”¨)
**é¢„æœŸæå‡**: 2-4x (ç›¸æ¯” CPU)

**æ–¹æ¡ˆé€‰æ‹©:**
- **DirectML**: Windows åŸç”Ÿ,ç®€å•æ˜“ç”¨
- **OpenVINO GPU**: è·¨å¹³å°,æ€§èƒ½æœ€ä¼˜
- **IPEX GPU**: PyTorch åŸç”Ÿé›†æˆ

**æ£€æµ‹ GPU:**
```python
import torch
print(torch.cuda.is_available())  # CUDA
# æˆ–ä½¿ç”¨ DirectML/OpenVINO æ£€æµ‹ Intel GPU
```

---

## æ€§èƒ½ç“¶é¢ˆåˆ†æ

### å½“å‰ç“¶é¢ˆ

1. **æ¨¡å‹åŠ è½½æ—¶é—´**: 2.8s (å¹³å‡)
   - åŸå› : ç£ç›˜ I/O (ç½‘ç»œé©±åŠ¨å™¨)
   - è§£å†³: ä½¿ç”¨æœ¬åœ° SSD

2. **æ¨ç†å»¶è¿Ÿ**: 37ms
   - åŸå› : CPU è®¡ç®—å¯†é›†
   - è§£å†³: IPEX ä¼˜åŒ– æˆ– OpenVINO

3. **æ‰¹é‡ååé‡**: 35 req/s
   - åŸå› : å•çº¿ç¨‹æ¨ç†
   - è§£å†³: å¢åŠ æ‰¹é‡å¤§å° æˆ– å¤šè¿›ç¨‹

### ä¼˜åŒ–ä¼˜å…ˆçº§

**P0 (ç«‹å³)**: âœ… å·²å®Œæˆ
- MKL çº¿ç¨‹ä¼˜åŒ–
- oneDNN å¯ç”¨

**P1 (1-2 å‘¨)**:
- IPEX é›†æˆ
- æ¨¡å‹æ–‡ä»¶è¿ç§»åˆ°æœ¬åœ° SSD

**P2 (2-4 å‘¨)**:
- OpenVINO è½¬æ¢
- INT8 é‡åŒ–

**P3 (é•¿æœŸ)**:
- GPU åŠ é€Ÿ
- NPU æ”¯æŒ (å¦‚æœç¡¬ä»¶æ”¯æŒ)

---

## ç¡¬ä»¶å»ºè®®

### å½“å‰ç¡¬ä»¶è¯„ä¼°

âœ… **CPU**: Intel 12th Gen+ (è‰¯å¥½)
- 12 ç‰©ç†æ ¸å¿ƒ
- æ”¯æŒ AVX-512 (éƒ¨åˆ†å‹å·)
- æ”¯æŒ MKL-DNN

âš ï¸ **å†…å­˜**: 15.7 GB (ä½¿ç”¨ç‡ 82%)
- å»ºè®®: å¢åŠ åˆ° 32GB
- åŸå› : æ”¯æŒæ›´å¤§æ‰¹é‡æ¨ç†

âš ï¸ **å­˜å‚¨**: ç½‘ç»œé©±åŠ¨å™¨
- å»ºè®®: è¿ç§»åˆ°æœ¬åœ° NVMe SSD
- åŸå› : å‡å°‘æ¨¡å‹åŠ è½½æ—¶é—´ (6s -> <1s)

â“ **GPU**: æœªæ£€æµ‹åˆ°
- å»ºè®®: æ£€æŸ¥æ˜¯å¦æœ‰ Intel Iris Xe / Arc
- ç”¨é€”: è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†

---

## æ–‡æ¡£å’Œèµ„æº

### å·²åˆ›å»ºæ–‡æ¡£
1. `INTEL_AI_ACCELERATION_GUIDE.md` - å®Œæ•´ä¼˜åŒ–æŒ‡å—
2. `ARROWENGINE_INTEL_OPTIMIZATION_SUMMARY.md` - æœ¬æ–‡æ¡£
3. `VALIDATION_REPORT.md` - æµ‹è¯•æŠ¥å‘Š

### å‚è€ƒèµ„æº
- [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/)
- [OpenVINO Toolkit](https://docs.openvino.ai/)
- [Intel Deep Learning Boost](https://www.intel.com/content/www/us/en/developer/articles/guide/deep-learning-with-avx512-and-dl-boost.html)
- [PyTorch CPU æ¨ç†ä¼˜åŒ–](https://pytorch.org/blog/accelerated-cpu-inference/)

---

## æ€»ç»“

### å·²å®Œæˆ âœ…
- Intel MKL çº¿ç¨‹ä¼˜åŒ–
- oneDNN å¯ç”¨
- ç¯å¢ƒå˜é‡é…ç½®
- å®Œæ•´éªŒè¯æµ‹è¯• (8/8 é€šè¿‡)
- æ€§èƒ½æå‡ 1.4-1.6x

### å¾…å®Œæˆ ğŸ“‹
- IPEX é›†æˆ (é¢„æœŸ 2-3x)
- OpenVINO è½¬æ¢ (é¢„æœŸ 3-5x)
- GPU åŠ é€Ÿæ¢ç´¢ (é¢„æœŸ 2-4x)

### æœ€ç»ˆç›®æ ‡ ğŸ¯
- æ¨ç†å»¶è¿Ÿ: < 10ms (å½“å‰ 37ms)
- æ‰¹é‡ååé‡: > 200 req/s (å½“å‰ 35 req/s)
- æ¨¡å‹åŠ è½½: < 500ms (å½“å‰ 2.8s)

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**: å®æ–½ IPEX ä¼˜åŒ– (æ–¹æ¡ˆ B)
