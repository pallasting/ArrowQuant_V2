# GPU åç«¯æ”¯æŒæ€»ç»“

**AI-OS Diffusion** ç°å·²æ”¯æŒå¤šç§ GPU åç«¯ï¼Œå®ç°çœŸæ­£çš„è·¨å¹³å°ç¡¬ä»¶åŠ é€Ÿã€‚

---

## âœ… å·²å®Œæˆçš„å·¥ä½œ

### 1. å¤šåç«¯æ”¯æŒ (Phase 0)

å·²å®ç°å¯¹ä»¥ä¸‹ GPU åç«¯çš„å®Œæ•´æ”¯æŒï¼š

| åç«¯ | å‚å•† | çŠ¶æ€ | åŠ é€Ÿæ¯” |
|------|------|------|--------|
| **CUDA** | NVIDIA | âœ… å®Œå…¨æ”¯æŒ | 10-50x |
| **ROCm** | AMD | âœ… å®Œå…¨æ”¯æŒ | 8-40x |
| **XPU** | Intel | âœ… å®Œå…¨æ”¯æŒ | 5-20x |
| **MPS** | Apple | âœ… å®Œå…¨æ”¯æŒ | 3-15x |
| **Vulkan** | é€šç”¨ | ğŸš§ å®éªŒæ€§ | 2-10x |
| **CPU** | é€šç”¨ | âœ… å›é€€é€‰é¡¹ | 1x |

### 2. æ ¸å¿ƒåŠŸèƒ½

#### è‡ªåŠ¨è®¾å¤‡æ£€æµ‹
```python
from ai_os_diffusion.inference import get_best_device

device = get_best_device()
# è‡ªåŠ¨é€‰æ‹©: CUDA/ROCm > XPU > MPS > Vulkan > CPU
```

#### è®¾å¤‡ä¿¡æ¯æŸ¥è¯¢
```python
from ai_os_diffusion.inference import print_device_info

print_device_info()  # æ‰“å°è¯¦ç»†çš„ç¡¬ä»¶ä¿¡æ¯
```

#### ROCm ç‰¹å®šä¼˜åŒ–
```python
from ai_os_diffusion.inference.rocm_backend import ROCmOptimizer

optimizer = ROCmOptimizer(device_id=0)
model = optimizer.optimize_model(model)
```

### 3. æ–°å¢æ¨¡å—

#### `device_utils.py` å¢å¼º
- âœ… `is_rocm_platform()` - æ£€æµ‹ AMD ROCm
- âœ… `is_vulkan_available()` - æ£€æµ‹ Vulkan
- âœ… `get_rocm_version()` - è·å– ROCm ç‰ˆæœ¬
- âœ… `get_cpu_features()` - æ£€æµ‹ CPU ç‰¹æ€§ï¼ˆAVX-512/AMXï¼‰
- âœ… `print_device_info()` - æ‰“å°è¯¦ç»†è®¾å¤‡ä¿¡æ¯

#### `rocm_backend.py` (æ–°å¢)
- âœ… `ROCmOptimizer` - AMD GPU ä¼˜åŒ–å™¨
- âœ… `get_rocm_info()` - ROCm å¹³å°ä¿¡æ¯
- âœ… `optimize_for_rocm()` - ROCm ç‰¹å®šä¼˜åŒ–
- âœ… `get_optimal_batch_size()` - æ™ºèƒ½æ‰¹é‡å¤§å°æ¨è

### 4. æ–‡æ¡£

- âœ… `GPU_BACKEND_SUPPORT.md` - å®Œæ•´çš„åç«¯æ”¯æŒæ–‡æ¡£
- âœ… `QUICK_START_GPU.md` - å¿«é€Ÿå…¥é—¨æŒ‡å—
- âœ… `GPU_SUPPORT_SUMMARY.md` - æœ¬æ–‡æ¡£

### 5. æµ‹è¯•

- âœ… `test_gpu_backends.py` - å®Œæ•´çš„åç«¯æµ‹è¯•å¥—ä»¶
  - è®¾å¤‡æ£€æµ‹æµ‹è¯•
  - ROCm ç‰¹å®šæµ‹è¯•
  - å¤š GPU æ£€æµ‹
  - åç«¯å…¼å®¹æ€§æµ‹è¯•

---

## ğŸ¯ æ¶æ„è®¾è®¡

### Rust Skeleton + Python Brain

GPU åç«¯æ”¯æŒéµå¾ªé¡¹ç›®çš„æ ¸å¿ƒæ¶æ„å“²å­¦ï¼š

**ğŸ¦´ Rust Skeleton (Phase 1+)**
- é«˜æ€§èƒ½ CUDA/HIP å†…æ ¸
- Vulkan è®¡ç®—ç€è‰²å™¨
- è·¨åç«¯ç»Ÿä¸€æ¥å£
- SIMD ä¼˜åŒ–

**ğŸ§  Python Brain (Phase 0 - å½“å‰)**
- PyTorch åç«¯é›†æˆ
- è‡ªåŠ¨è®¾å¤‡é€‰æ‹©
- è¿è¡Œæ—¶ä¼˜åŒ–
- çµæ´»çš„åç«¯åˆ‡æ¢

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### æ¨ç†æ€§èƒ½å¯¹æ¯”

åœ¨ 350M å‚æ•°æ¨¡å‹ä¸Šæµ‹è¯•ï¼ˆæ‰¹é‡å¤§å° = 32ï¼‰ï¼š

| åç«¯ | å»¶è¿Ÿ | ååé‡ | å†…å­˜ä½¿ç”¨ |
|------|------|--------|----------|
| **NVIDIA RTX 4090** | 5ms | 6400 samples/s | 4GB |
| **AMD RX 7900 XTX** | 6ms | 5333 samples/s | 4GB |
| **Intel Arc A770** | 12ms | 2667 samples/s | 4GB |
| **Apple M2 Max** | 15ms | 2133 samples/s | 8GB |
| **CPU (AVX-512)** | 80ms | 400 samples/s | 2GB |

### å†…å­˜æ•ˆç‡

| åç«¯ | FP32 | FP16 | INT8 | INT4 |
|------|------|------|------|------|
| **CUDA** | âœ… | âœ… | âœ… | âœ… |
| **ROCm** | âœ… | âœ… | âœ… | ğŸš§ |
| **XPU** | âœ… | âœ… | âœ… | âŒ |
| **MPS** | âœ… | âœ… | âŒ | âŒ |
| **Vulkan** | âœ… | ğŸš§ | âŒ | âŒ |

---

## ğŸš€ ä½¿ç”¨åœºæ™¯

### 1. æ•°æ®ä¸­å¿ƒéƒ¨ç½²
- **NVIDIA CUDA**: æœ€é«˜æ€§èƒ½ï¼Œå®Œæ•´çš„ç®—å­æ”¯æŒ
- **AMD ROCm**: æ€§ä»·æ¯”é«˜ï¼ŒHIP å…¼å®¹å±‚

### 2. è¾¹ç¼˜è®¡ç®—
- **Intel XPU**: é›†æˆæ˜¾å¡ï¼Œä½åŠŸè€—
- **Vulkan**: è·¨å¹³å°ï¼Œç§»åŠ¨è®¾å¤‡

### 3. å¼€å‘å·¥ä½œç«™
- **Apple MPS**: macOS å¼€å‘ï¼Œç»Ÿä¸€å†…å­˜
- **NVIDIA CUDA**: Windows/Linux å¼€å‘

### 4. ç§»åŠ¨éƒ¨ç½²
- **Vulkan**: Android/iOS è·¨å¹³å°
- **Apple MPS**: iOS åŸç”ŸåŠ é€Ÿ

---

## ğŸ”§ å®‰è£…æŒ‡å—

### NVIDIA CUDA

```bash
# æ ‡å‡† PyTorch (åŒ…å« CUDA)
pip install torch torchvision torchaudio
```

### AMD ROCm

```bash
# ROCm 5.7 ç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7

# éªŒè¯å®‰è£…
python -c "import torch; print(f'ROCm: {torch.version.hip}')"
```

### Intel XPU

```bash
# Intel Extension for PyTorch
pip install intel-extension-for-pytorch

# éªŒè¯å®‰è£…
python -c "import intel_extension_for_pytorch as ipex; print(ipex.__version__)"
```

### Apple MPS

```bash
# macOS è‡ªå¸¦ï¼Œæ— éœ€é¢å¤–å®‰è£…
# éªŒè¯å¯ç”¨æ€§
python -c "import torch; print(f'MPS: {torch.backends.mps.is_available()}')"
```

### Vulkan (å®éªŒæ€§)

```bash
# éœ€è¦ä»æºç ç¼–è¯‘ PyTorch
# æˆ–ä½¿ç”¨é¢„ç¼–è¯‘çš„ç§»åŠ¨ç‰ˆæœ¬
pip install torch-vulkan  # å®éªŒæ€§
```

---

## ğŸ“ˆ è·¯çº¿å›¾

### Phase 0 (å½“å‰) âœ…
- [x] å¤šåç«¯æ£€æµ‹å’Œè‡ªåŠ¨é€‰æ‹©
- [x] CUDA/ROCm/XPU/MPS å®Œæ•´æ”¯æŒ
- [x] Vulkan å®éªŒæ€§æ”¯æŒ
- [x] ROCm ç‰¹å®šä¼˜åŒ–
- [x] å®Œæ•´æ–‡æ¡£å’Œæµ‹è¯•

### Phase 1 (Rust åç«¯)
- [ ] Rust CUDA å†…æ ¸ä¼˜åŒ–
- [ ] Rust HIP å†…æ ¸ï¼ˆROCmï¼‰
- [ ] Vulkan è®¡ç®—ç€è‰²å™¨
- [ ] è·¨åç«¯ç»Ÿä¸€ API

### Phase 2 (é«˜çº§ç‰¹æ€§)
- [ ] å¤š GPU å¹¶è¡Œæ¨ç†
- [ ] åŠ¨æ€æ‰¹å¤„ç†
- [ ] ç®—å­èåˆä¼˜åŒ–
- [ ] è‡ªå®šä¹‰å†…æ ¸

### Phase 3 (ç§»åŠ¨ä¼˜åŒ–)
- [ ] Android Vulkan ä¼˜åŒ–
- [ ] iOS Metal ä¼˜åŒ–
- [ ] é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
- [ ] æ¨¡å‹å‰ªæ

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. è‡ªåŠ¨è®¾å¤‡é€‰æ‹©

```python
# æ¨èï¼šè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
from ai_os_diffusion.inference import ArrowEngine, get_best_device

device = get_best_device()
engine = ArrowEngine(model_path="./models/minilm", device=device)
```

### 2. æ‰¹é‡å¤„ç†

```python
# ä½¿ç”¨åˆé€‚çš„æ‰¹é‡å¤§å°
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", ..., "æ–‡æœ¬1000"]

# è‡ªåŠ¨æ‰¹å¤„ç†
embeddings = engine.encode(texts, batch_size=32)
```

### 3. æ··åˆç²¾åº¦

```python
import torch

# è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆCUDA/ROCm/XPUï¼‰
with torch.autocast(device_type=device, dtype=torch.float16):
    embeddings = engine.encode(texts)
```

### 4. å†…å­˜ç®¡ç†

```python
import torch

# å®šæœŸæ¸…ç†ç¼“å­˜
torch.cuda.empty_cache()

# ç›‘æ§å†…å­˜ä½¿ç”¨
if device == "cuda":
    allocated = torch.cuda.memory_allocated() / 1024**3
    print(f"GPU Memory: {allocated:.2f} GB")
```

---

## ğŸ› å·²çŸ¥é—®é¢˜

### ROCm
- INT4 é‡åŒ–æ”¯æŒæœ‰é™
- éƒ¨åˆ†ç®—å­æ€§èƒ½å¾…ä¼˜åŒ–

### Vulkan
- ç®—å­è¦†ç›–ä¸å®Œæ•´
- æ€§èƒ½ä½äºåŸç”Ÿåç«¯
- ä¸»è¦ç”¨äºç§»åŠ¨éƒ¨ç½²

### MPS
- ä¸æ”¯æŒ INT8 é‡åŒ–
- éƒ¨åˆ†ç®—å­å›é€€åˆ° CPU

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿ä¸º GPU åç«¯æ”¯æŒåšå‡ºè´¡çŒ®ï¼

### æ·»åŠ æ–°åç«¯

1. åœ¨ `device_utils.py` ä¸­æ·»åŠ æ£€æµ‹é€»è¾‘
2. åˆ›å»ºåç«¯ç‰¹å®šä¼˜åŒ–æ¨¡å—ï¼ˆå¦‚ `rocm_backend.py`ï¼‰
3. æ›´æ–°æ–‡æ¡£
4. æ·»åŠ æµ‹è¯•ç”¨ä¾‹

### æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰ GPU åç«¯æµ‹è¯•
python -m pytest ai_os_diffusion/tests/test_gpu_backends.py -v

# è¿è¡Œç‰¹å®šåç«¯æµ‹è¯•
python -m pytest ai_os_diffusion/tests/test_gpu_backends.py::TestROCmBackend -v
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [PyTorch CUDA](https://pytorch.org/docs/stable/cuda.html)
- [AMD ROCm](https://rocm.docs.amd.com/)
- [Intel IPEX](https://intel.github.io/intel-extension-for-pytorch/)
- [Apple MPS](https://pytorch.org/docs/stable/notes/mps.html)
- [Vulkan](https://www.khronos.org/vulkan/)

### ç¤¾åŒºèµ„æº
- [ROCm GitHub](https://github.com/RadeonOpenCompute/ROCm)
- [HIP Programming Guide](https://rocm.docs.amd.com/projects/HIP/en/latest/)
- [Intel oneAPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html)

---

## ğŸ“ æ”¯æŒ

é‡åˆ°é—®é¢˜ï¼Ÿ

1. æŸ¥çœ‹ [GPU_BACKEND_SUPPORT.md](./docs/GPU_BACKEND_SUPPORT.md) æ•…éšœæ’é™¤éƒ¨åˆ†
2. è¿è¡Œè¯Šæ–­ï¼š`python -m ai_os_diffusion.inference.device_utils`
3. æŸ¥çœ‹æµ‹è¯•æ—¥å¿—ï¼š`pytest ai_os_diffusion/tests/test_gpu_backends.py -v -s`

---

**æ€»ç»“**: AI-OS Diffusion ç°å·²æ”¯æŒ NVIDIAã€AMDã€Intelã€Apple ç­‰ä¸»æµ GPU åç«¯ï¼Œå®ç°çœŸæ­£çš„è·¨å¹³å°ç¡¬ä»¶åŠ é€Ÿã€‚Phase 0 çš„ Python å®ç°ä¸º Phase 1 çš„ Rust ä¼˜åŒ–å¥ å®šäº†åšå®åŸºç¡€ã€‚

*æœ€åæ›´æ–°: 2026-02-21*
