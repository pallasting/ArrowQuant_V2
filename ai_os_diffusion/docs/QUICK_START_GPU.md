# GPU åç«¯å¿«é€Ÿå…¥é—¨

å¿«é€Ÿå¼€å§‹ä½¿ç”¨ AI-OS Diffusion çš„å¤š GPU åç«¯æ”¯æŒã€‚

---

## å¿«é€Ÿæ£€æµ‹

### æ£€æµ‹å¯ç”¨çš„ GPU åç«¯

```python
from ai_os_diffusion.inference import print_device_info

# æ‰“å°è¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
print_device_info()
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
============================================================
Device Information
============================================================
Device Type: cuda
Backend: CUDA (NVIDIA)
Name: NVIDIA GeForce RTX 4090
Memory: 24.00 GB
CUDA Version: 12.1
============================================================
```

---

## æ”¯æŒçš„ç¡¬ä»¶

| å‚å•† | åç«¯ | çŠ¶æ€ | å®‰è£… |
|------|------|------|------|
| **NVIDIA** | CUDA | âœ… å®Œå…¨æ”¯æŒ | `pip install torch` |
| **AMD** | ROCm | âœ… å®Œå…¨æ”¯æŒ | `pip install torch --index-url https://download.pytorch.org/whl/rocm5.7` |
| **Intel** | XPU | âœ… å®Œå…¨æ”¯æŒ | `pip install intel-extension-for-pytorch` |
| **Apple** | MPS | âœ… å®Œå…¨æ”¯æŒ | å†…ç½®äº PyTorch (macOS) |
| **é€šç”¨** | Vulkan | ğŸš§ å®éªŒæ€§ | éœ€è¦ä»æºç ç¼–è¯‘ |

---

## ä½¿ç”¨ç¤ºä¾‹

### 1. è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡

```python
from ai_os_diffusion.inference import ArrowEngine, get_best_device

# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æœ€ä½³è®¾å¤‡
device = get_best_device()
print(f"Using: {device}")

# åˆå§‹åŒ–å¼•æ“
engine = ArrowEngine(
    model_path="./models/minilm",
    device=device
)

# æ¨ç†
embeddings = engine.encode(["ä½ å¥½ï¼Œä¸–ç•Œï¼"])
```

### 2. æŒ‡å®šç‰¹å®šè®¾å¤‡

```python
# å¼ºåˆ¶ä½¿ç”¨ CUDA (NVIDIA/AMD)
engine = ArrowEngine(model_path="./models/minilm", device="cuda")

# å¼ºåˆ¶ä½¿ç”¨ Intel XPU
engine = ArrowEngine(model_path="./models/minilm", device="xpu")

# å¼ºåˆ¶ä½¿ç”¨ Apple MPS
engine = ArrowEngine(model_path="./models/minilm", device="mps")

# å›é€€åˆ° CPU
engine = ArrowEngine(model_path="./models/minilm", device="cpu")
```

### 3. AMD ROCm ç‰¹å®šä¼˜åŒ–

```python
from ai_os_diffusion.inference.rocm_backend import ROCmOptimizer

# åˆå§‹åŒ– ROCm ä¼˜åŒ–å™¨
optimizer = ROCmOptimizer(device_id=0)

# ä¼˜åŒ–æ¨¡å‹
model = optimizer.optimize_model(model, enable_fusion=True)

# è·å–æ¨èçš„æ‰¹é‡å¤§å°
batch_size = optimizer.get_recommended_batch_size(
    model_size_mb=100,
    sequence_length=512
)
print(f"Recommended batch size: {batch_size}")
```

### 4. æ£€æµ‹ ROCm å¹³å°

```python
from ai_os_diffusion.inference import is_rocm_platform

if is_rocm_platform():
    print("Running on AMD GPU with ROCm")
    from ai_os_diffusion.inference.rocm_backend import print_rocm_info
    print_rocm_info()
else:
    print("Not running on ROCm")
```

---

## æ€§èƒ½å¯¹æ¯”

### æ¨ç†é€Ÿåº¦ï¼ˆç›¸å¯¹äº CPUï¼‰

```python
import time
from ai_os_diffusion.inference import ArrowEngine

texts = ["æµ‹è¯•æ–‡æœ¬"] * 100

# CPU åŸºå‡†
engine_cpu = ArrowEngine("./models/minilm", device="cpu")
start = time.time()
embeddings = engine_cpu.encode(texts)
cpu_time = time.time() - start
print(f"CPU: {cpu_time:.2f}s")

# GPU åŠ é€Ÿ
engine_gpu = ArrowEngine("./models/minilm", device="cuda")
start = time.time()
embeddings = engine_gpu.encode(texts)
gpu_time = time.time() - start
print(f"GPU: {gpu_time:.2f}s")
print(f"Speedup: {cpu_time/gpu_time:.1f}x")
```

é¢„æœŸåŠ é€Ÿæ¯”ï¼š
- NVIDIA CUDA: 10-50x
- AMD ROCm: 8-40x
- Intel XPU: 5-20x
- Apple MPS: 3-15x

---

## å¤š GPU æ”¯æŒ

### æ£€æµ‹å¤šä¸ª GPU

```python
import torch

if torch.cuda.is_available():
    gpu_count = torch.cuda.device_count()
    print(f"Found {gpu_count} GPU(s)")
    
    for i in range(gpu_count):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
```

### ä½¿ç”¨ç‰¹å®š GPU

```python
# ä½¿ç”¨ç¬¬äºŒä¸ª GPU
engine = ArrowEngine(
    model_path="./models/minilm",
    device="cuda:1"  # æŒ‡å®š GPU ID
)
```

---

## æ•…éšœæ’é™¤

### CUDA å†…å­˜ä¸è¶³

```python
# æ–¹æ¡ˆ 1: å‡å°‘æ‰¹é‡å¤§å°
engine = ArrowEngine(model_path="./models/minilm", device="cuda")
embeddings = engine.encode(texts, batch_size=8)  # å‡å°æ‰¹é‡

# æ–¹æ¡ˆ 2: ä½¿ç”¨é‡åŒ–
engine = ArrowEngine(
    model_path="./models/minilm",
    device="cuda",
    quantization="int8"  # ä½¿ç”¨ INT8 é‡åŒ–
)

# æ–¹æ¡ˆ 3: æ¸…ç†ç¼“å­˜
import torch
torch.cuda.empty_cache()
```

### ROCm æœªæ£€æµ‹åˆ°

```bash
# æ£€æŸ¥ ROCm å®‰è£…
rocm-smi

# é‡æ–°å®‰è£… PyTorch ROCm ç‰ˆæœ¬
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/rocm5.7
```

### Intel XPU æœªæ£€æµ‹åˆ°

```bash
# å®‰è£… Intel Extension for PyTorch
pip install intel-extension-for-pytorch

# éªŒè¯å®‰è£…
python -c "import intel_extension_for_pytorch as ipex; print(ipex.__version__)"
```

### Apple MPS å†…å­˜ä¸è¶³

```python
import os

# å¯ç”¨ MPS å›é€€åˆ° CPU
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

engine = ArrowEngine(model_path="./models/minilm", device="mps")
```

---

## ç¯å¢ƒå˜é‡

### CUDA/ROCm

```bash
# æŒ‡å®šå¯è§çš„ GPU
export CUDA_VISIBLE_DEVICES=0,1

# è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### Intel XPU

```bash
# å¯ç”¨ IPEX ä¼˜åŒ–
export IPEX_TILE_AS_DEVICE=1
```

### é€šç”¨

```bash
# ç¦ç”¨ GPUï¼ˆå¼ºåˆ¶ä½¿ç”¨ CPUï¼‰
export CUDA_VISIBLE_DEVICES=""
```

---

## åŸºå‡†æµ‹è¯•

è¿è¡Œå®Œæ•´çš„ GPU åç«¯æµ‹è¯•ï¼š

```bash
# æµ‹è¯•æ‰€æœ‰åç«¯
python -m pytest ai_os_diffusion/tests/test_gpu_backends.py -v -s

# åªæµ‹è¯•è®¾å¤‡æ£€æµ‹
python -m pytest ai_os_diffusion/tests/test_gpu_backends.py::TestGPUBackends::test_device_detection -v

# æµ‹è¯• ROCm åç«¯
python -m pytest ai_os_diffusion/tests/test_gpu_backends.py::TestROCmBackend -v
```

---

## æ›´å¤šä¿¡æ¯

è¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒï¼š
- [GPU Backend Support](./GPU_BACKEND_SUPPORT.md) - å®Œæ•´çš„åç«¯æ”¯æŒæ–‡æ¡£
- [NVIDIA CUDA æ–‡æ¡£](https://pytorch.org/docs/stable/cuda.html)
- [AMD ROCm æ–‡æ¡£](https://rocm.docs.amd.com/)
- [Intel IPEX æ–‡æ¡£](https://intel.github.io/intel-extension-for-pytorch/)
- [Apple MPS æ–‡æ¡£](https://pytorch.org/docs/stable/notes/mps.html)

---

*æœ€åæ›´æ–°: 2026-02-21*
