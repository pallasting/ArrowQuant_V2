# GPU Backend Support

**AI-OS Diffusion** æ”¯æŒå¤šç§ GPU åç«¯ï¼Œå®ç°è·¨å¹³å°ç¡¬ä»¶åŠ é€Ÿã€‚

---

## æ”¯æŒçš„ç¡¬ä»¶åç«¯

### 1. NVIDIA GPU (CUDA) âœ…
**çŠ¶æ€**: å®Œå…¨æ”¯æŒ  
**åç«¯**: CUDA 11.8+ / CUDA 12.x  
**æ¡†æ¶**: PyTorch CUDA

**ç‰¹æ€§**:
- Tensor Cores åŠ é€Ÿï¼ˆFP16/BF16ï¼‰
- CUDA Graphs ä¼˜åŒ–
- å¤š GPU æ”¯æŒ
- é›¶æ‹·è´å†…å­˜ä¼ è¾“

**æ£€æµ‹**:
```python
import torch
if torch.cuda.is_available():
    print(f"CUDA Device: {torch.cuda.get_device_name(0)}")
```

---

### 2. AMD GPU (ROCm) âœ…
**çŠ¶æ€**: å®Œå…¨æ”¯æŒ  
**åç«¯**: ROCm 5.4+ (é€šè¿‡ HIP)  
**æ¡†æ¶**: PyTorch ROCm

**ç‰¹æ€§**:
- é€šè¿‡ HIP å…¼å®¹å±‚ä½¿ç”¨ CUDA æ¥å£
- Matrix Core åŠ é€Ÿï¼ˆCDNA æ¶æ„ï¼‰
- å¤š GPU æ”¯æŒ
- ä¸ CUDA ä»£ç å…¼å®¹

**æ£€æµ‹**:
```python
import torch
if torch.cuda.is_available():
    if hasattr(torch.version, "hip") and torch.version.hip:
        print(f"ROCm Device: {torch.cuda.get_device_name(0)}")
        print(f"HIP Version: {torch.version.hip}")
```

**æ”¯æŒçš„ GPU**:
- AMD Radeon RX 6000/7000 ç³»åˆ—
- AMD Instinct MI100/MI200/MI300 ç³»åˆ—
- AMD Radeon Pro ç³»åˆ—

**å®‰è£…**:
```bash
# ROCm PyTorch (Linux)
pip install torch torchvision --index-url https://download.pytorch.org/whl/rocm5.7
```

---

### 3. Intel GPU (XPU) âœ…
**çŠ¶æ€**: å®Œå…¨æ”¯æŒ  
**åç«¯**: Intel Extension for PyTorch (IPEX)  
**æ¡†æ¶**: PyTorch + IPEX

**ç‰¹æ€§**:
- XMX çŸ©é˜µå¼•æ“åŠ é€Ÿ
- Arc/Flex/Max GPU æ”¯æŒ
- CPU + GPU æ··åˆæ¨ç†
- ä¼˜åŒ–çš„ç®—å­èåˆ

**æ£€æµ‹**:
```python
import torch
import intel_extension_for_pytorch as ipex

if torch.xpu.is_available():
    print(f"Intel GPU: {torch.xpu.get_device_name(0)}")
```

**æ”¯æŒçš„ GPU**:
- Intel Arc A-Series (æ¶ˆè´¹çº§)
- Intel Data Center GPU Flex/Max (æ•°æ®ä¸­å¿ƒ)
- Intel Iris Xe (é›†æˆæ˜¾å¡)

**å®‰è£…**:
```bash
# Intel Extension for PyTorch
pip install intel-extension-for-pytorch
```

---

### 4. Apple Silicon (MPS) âœ…
**çŠ¶æ€**: å®Œå…¨æ”¯æŒ  
**åç«¯**: Metal Performance Shaders  
**æ¡†æ¶**: PyTorch MPS

**ç‰¹æ€§**:
- ç»Ÿä¸€å†…å­˜æ¶æ„
- Neural Engine åŠ é€Ÿ
- ä½åŠŸè€—é«˜æ€§èƒ½
- M1/M2/M3 ç³»åˆ—æ”¯æŒ

**æ£€æµ‹**:
```python
import torch
if torch.backends.mps.is_available():
    print("Apple Silicon MPS available")
```

**æ”¯æŒçš„èŠ¯ç‰‡**:
- Apple M1/M1 Pro/M1 Max/M1 Ultra
- Apple M2/M2 Pro/M2 Max/M2 Ultra
- Apple M3/M3 Pro/M3 Max

---

### 5. Vulkan (è·¨å¹³å°) ğŸš§
**çŠ¶æ€**: å®éªŒæ€§æ”¯æŒ  
**åç«¯**: Vulkan Compute  
**æ¡†æ¶**: PyTorch Vulkan Backend

**ç‰¹æ€§**:
- è·¨å¹³å° GPU åŠ é€Ÿ
- ç§»åŠ¨è®¾å¤‡æ”¯æŒï¼ˆAndroid/iOSï¼‰
- ä½çº§ GPU æ§åˆ¶
- å¤šå‚å•†å…¼å®¹

**æ£€æµ‹**:
```python
import torch
if hasattr(torch, "vulkan") and torch.vulkan.is_available():
    print("Vulkan backend available")
```

**æ”¯æŒçš„å¹³å°**:
- Windows (NVIDIA/AMD/Intel GPU)
- Linux (NVIDIA/AMD/Intel GPU)
- Android (Qualcomm Adreno, ARM Mali)
- iOS (Apple GPU)

**å®‰è£…**:
```bash
# PyTorch with Vulkan support (éœ€è¦ä»æºç ç¼–è¯‘)
# æˆ–ä½¿ç”¨é¢„ç¼–è¯‘çš„ç§»åŠ¨ç‰ˆæœ¬
pip install torch-vulkan  # å®éªŒæ€§
```

**é™åˆ¶**:
- ç®—å­è¦†ç›–ä¸å®Œæ•´
- æ€§èƒ½å¯èƒ½ä½äºåŸç”Ÿåç«¯
- ä¸»è¦ç”¨äºç§»åŠ¨éƒ¨ç½²

---

## è‡ªåŠ¨è®¾å¤‡é€‰æ‹©

`device_utils.py` å®ç°äº†æ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼š

```python
from ai_os_diffusion.inference import get_best_device

device = get_best_device()
# ä¼˜å…ˆçº§: CUDA/ROCm > XPU > MPS > Vulkan > CPU
```

**é€‰æ‹©é€»è¾‘**:
1. **CUDA/ROCm**: æœ€é«˜ä¼˜å…ˆçº§ï¼ˆNVIDIA æˆ– AMD GPUï¼‰
2. **XPU**: Intel GPUï¼ˆå¦‚æœå®‰è£…äº† IPEXï¼‰
3. **MPS**: Apple Siliconï¼ˆmacOSï¼‰
4. **Vulkan**: è·¨å¹³å° GPUï¼ˆå®éªŒæ€§ï¼‰
5. **CPU**: å›é€€é€‰é¡¹ï¼ˆæ”¯æŒ AVX-512/AMXï¼‰

---

## æ€§èƒ½å¯¹æ¯”

### æ¨ç†æ€§èƒ½ï¼ˆç›¸å¯¹äº CPUï¼‰

| åç«¯ | åŠ é€Ÿæ¯” | ç²¾åº¦ | åŠŸè€— | é€‚ç”¨åœºæ™¯ |
|------|--------|------|------|----------|
| **CUDA (NVIDIA)** | 10-50x | FP16/BF16 | é«˜ | æ•°æ®ä¸­å¿ƒã€å·¥ä½œç«™ |
| **ROCm (AMD)** | 8-40x | FP16/BF16 | é«˜ | æ•°æ®ä¸­å¿ƒã€å·¥ä½œç«™ |
| **XPU (Intel)** | 5-20x | FP16/INT8 | ä¸­ | è¾¹ç¼˜è®¡ç®—ã€æœåŠ¡å™¨ |
| **MPS (Apple)** | 3-15x | FP16 | ä½ | ç¬”è®°æœ¬ã€ç§»åŠ¨è®¾å¤‡ |
| **Vulkan** | 2-10x | FP32 | ä¸­ | è·¨å¹³å°ã€ç§»åŠ¨ |
| **CPU (AVX-512)** | 1x | FP32/INT8 | ä½ | é€šç”¨ã€è¾¹ç¼˜ |

### å†…å­˜æ•ˆç‡

| åç«¯ | é‡åŒ–æ”¯æŒ | æœ€å°å†…å­˜ | æ¨èå†…å­˜ |
|------|----------|----------|----------|
| **CUDA** | INT8/INT4/INT2 | 4GB | 8GB+ |
| **ROCm** | INT8/INT4 | 4GB | 8GB+ |
| **XPU** | INT8/INT4 | 4GB | 8GB+ |
| **MPS** | FP16 | 8GB | 16GB+ |
| **Vulkan** | FP32 | 2GB | 4GB+ |
| **CPU** | INT8/INT4 | 2GB | 4GB+ |

---

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from ai_os_diffusion.inference import ArrowEngine, get_best_device

# è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
device = get_best_device()
print(f"Using device: {device}")

# åˆå§‹åŒ–å¼•æ“
engine = ArrowEngine(
    model_path="./models/minilm",
    device=device
)

# æ¨ç†
embeddings = engine.encode(["Hello, world!"])
```

### æŒ‡å®šè®¾å¤‡

```python
# å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šè®¾å¤‡
engine = ArrowEngine(
    model_path="./models/minilm",
    device="cuda"  # æˆ– "xpu", "mps", "vulkan", "cpu"
)
```

### å¤š GPU æ”¯æŒ

```python
import torch

# CUDA/ROCm å¤š GPU
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs")
    # DataParallel æˆ– DistributedDataParallel
```

### æ··åˆç²¾åº¦

```python
# è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆCUDA/ROCm/XPUï¼‰
with torch.autocast(device_type=device, dtype=torch.float16):
    embeddings = engine.encode(texts)
```

---

## æ•…éšœæ’é™¤

### CUDA/ROCm é—®é¢˜

**é—®é¢˜**: `RuntimeError: CUDA out of memory`
```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘æ‰¹é‡å¤§å°æˆ–ä½¿ç”¨é‡åŒ–
engine = ArrowEngine(
    model_path="./models/minilm",
    device="cuda",
    quantization="int8"  # å‡å°‘å†…å­˜ä½¿ç”¨
)
```

### Intel XPU é—®é¢˜

**é—®é¢˜**: `torch.xpu not available`
```bash
# å®‰è£… Intel Extension for PyTorch
pip install intel-extension-for-pytorch
```

### Apple MPS é—®é¢˜

**é—®é¢˜**: `MPS backend out of memory`
```python
# ä½¿ç”¨ CPU å›é€€
import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
```

### Vulkan é—®é¢˜

**é—®é¢˜**: `Vulkan backend not available`
```bash
# ç¡®ä¿å®‰è£…äº† Vulkan é©±åŠ¨
# Windows: æ›´æ–°æ˜¾å¡é©±åŠ¨
# Linux: sudo apt install vulkan-tools
```

---

## å¼€å‘è·¯çº¿å›¾

### Phase 0 (å½“å‰) âœ…
- [x] CUDA/ROCm åŸºç¡€æ”¯æŒ
- [x] Intel XPU æ”¯æŒ
- [x] Apple MPS æ”¯æŒ
- [x] Vulkan å®éªŒæ€§æ”¯æŒ
- [x] è‡ªåŠ¨è®¾å¤‡é€‰æ‹©

### Phase 1 (Rust åç«¯)
- [ ] Rust CUDA å†…æ ¸ä¼˜åŒ–
- [ ] Rust ROCm HIP å†…æ ¸
- [ ] Rust Vulkan è®¡ç®—ç€è‰²å™¨
- [ ] è·¨åç«¯ç»Ÿä¸€æ¥å£

### Phase 2 (é«˜çº§ç‰¹æ€§)
- [ ] å¤š GPU å¹¶è¡Œæ¨ç†
- [ ] åŠ¨æ€æ‰¹å¤„ç†
- [ ] ç®—å­èåˆä¼˜åŒ–
- [ ] è‡ªå®šä¹‰ CUDA/HIP å†…æ ¸

### Phase 3 (ç§»åŠ¨éƒ¨ç½²)
- [ ] Android Vulkan ä¼˜åŒ–
- [ ] iOS Metal ä¼˜åŒ–
- [ ] é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
- [ ] æ¨¡å‹å‰ªæ

---

## è´¡çŒ®æŒ‡å—

æ¬¢è¿ä¸ºæ–°çš„ GPU åç«¯è´¡çŒ®ä»£ç ï¼

### æ·»åŠ æ–°åç«¯

1. åœ¨ `device_utils.py` ä¸­æ·»åŠ æ£€æµ‹é€»è¾‘
2. åœ¨ `inference_core.py` ä¸­æ·»åŠ åç«¯ç‰¹å®šä¼˜åŒ–
3. æ›´æ–°æ­¤æ–‡æ¡£
4. æ·»åŠ æµ‹è¯•ç”¨ä¾‹

### æµ‹è¯•

```bash
# æµ‹è¯•æ‰€æœ‰å¯ç”¨åç«¯
python -m pytest ai_os_diffusion/tests/test_device_backends.py -v
```

---

## å‚è€ƒèµ„æº

### CUDA/ROCm
- [PyTorch CUDA æ–‡æ¡£](https://pytorch.org/docs/stable/cuda.html)
- [AMD ROCm æ–‡æ¡£](https://rocm.docs.amd.com/)
- [HIP ç¼–ç¨‹æŒ‡å—](https://rocm.docs.amd.com/projects/HIP/en/latest/)

### Intel XPU
- [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/)
- [Intel GPU ä¼˜åŒ–æŒ‡å—](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-guide-gpu.html)

### Apple MPS
- [PyTorch MPS æ–‡æ¡£](https://pytorch.org/docs/stable/notes/mps.html)
- [Metal Performance Shaders](https://developer.apple.com/metal/pytorch/)

### Vulkan
- [PyTorch Vulkan åç«¯](https://pytorch.org/tutorials/prototype/vulkan_workflow.html)
- [Vulkan è®¡ç®—æ•™ç¨‹](https://www.khronos.org/vulkan/)

---

*æœ€åæ›´æ–°: 2026-02-21*
