{
  "model_name": "sentence-transformers/all-MiniLM-L6-v2",
  "model_type": "bert",
  "model_info": {
    "architecture": "BertModel",
    "embedding_dimension": 384,
    "hidden_size": 384,
    "num_attention_heads": 12,
    "intermediate_size": 1536,
    "num_hidden_layers": 6,
    "vocab_size": 30522,
    "max_position_embeddings": 512,
    "layer_norm_eps": 1e-12
  },
  "conversion_config": {
    "compression": "zstd",
    "compression_level": 3,
    "use_float16": false,
    "extract_tokenizer": true,
    "validate_output": true
  },
  "total_parameters": 22713216,
  "num_layers": 103,
  "layer_info": {
    "embeddings.word_embeddings.weight": {
      "shape": [
        30522,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 11720448
    },
    "embeddings.position_embeddings.weight": {
      "shape": [
        512,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 196608
    },
    "embeddings.token_type_embeddings.weight": {
      "shape": [
        2,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 768
    },
    "embeddings.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "embeddings.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.attention.self.query.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.0.attention.self.query.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.attention.self.key.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.0.attention.self.key.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.attention.self.value.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.0.attention.self.value.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.attention.output.dense.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.0.attention.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.attention.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.attention.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.intermediate.dense.weight": {
      "shape": [
        1536,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.0.intermediate.dense.bias": {
      "shape": [
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 1536
    },
    "encoder.layer.0.output.dense.weight": {
      "shape": [
        384,
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.0.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.0.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.attention.self.query.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.1.attention.self.query.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.attention.self.key.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.1.attention.self.key.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.attention.self.value.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.1.attention.self.value.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.attention.output.dense.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.1.attention.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.attention.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.attention.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.intermediate.dense.weight": {
      "shape": [
        1536,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.1.intermediate.dense.bias": {
      "shape": [
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 1536
    },
    "encoder.layer.1.output.dense.weight": {
      "shape": [
        384,
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.1.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.1.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.attention.self.query.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.2.attention.self.query.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.attention.self.key.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.2.attention.self.key.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.attention.self.value.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.2.attention.self.value.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.attention.output.dense.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.2.attention.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.attention.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.attention.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.intermediate.dense.weight": {
      "shape": [
        1536,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.2.intermediate.dense.bias": {
      "shape": [
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 1536
    },
    "encoder.layer.2.output.dense.weight": {
      "shape": [
        384,
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.2.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.2.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.attention.self.query.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.3.attention.self.query.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.attention.self.key.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.3.attention.self.key.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.attention.self.value.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.3.attention.self.value.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.attention.output.dense.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.3.attention.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.attention.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.attention.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.intermediate.dense.weight": {
      "shape": [
        1536,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.3.intermediate.dense.bias": {
      "shape": [
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 1536
    },
    "encoder.layer.3.output.dense.weight": {
      "shape": [
        384,
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.3.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.3.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.attention.self.query.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.4.attention.self.query.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.attention.self.key.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.4.attention.self.key.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.attention.self.value.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.4.attention.self.value.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.attention.output.dense.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.4.attention.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.attention.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.attention.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.intermediate.dense.weight": {
      "shape": [
        1536,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.4.intermediate.dense.bias": {
      "shape": [
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 1536
    },
    "encoder.layer.4.output.dense.weight": {
      "shape": [
        384,
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.4.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.4.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.attention.self.query.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.5.attention.self.query.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.attention.self.key.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.5.attention.self.key.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.attention.self.value.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.5.attention.self.value.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.attention.output.dense.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "encoder.layer.5.attention.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.attention.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.attention.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.intermediate.dense.weight": {
      "shape": [
        1536,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.5.intermediate.dense.bias": {
      "shape": [
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 1536
    },
    "encoder.layer.5.output.dense.weight": {
      "shape": [
        384,
        1536
      ],
      "dtype": "torch.float32",
      "num_params": 589824
    },
    "encoder.layer.5.output.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.output.LayerNorm.weight": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "encoder.layer.5.output.LayerNorm.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    },
    "pooler.dense.weight": {
      "shape": [
        384,
        384
      ],
      "dtype": "torch.float32",
      "num_params": 147456
    },
    "pooler.dense.bias": {
      "shape": [
        384
      ],
      "dtype": "torch.float32",
      "num_params": 384
    }
  },
  "converted_at": "2026-02-20T19:06:33.650709",
  "converter_version": "0.1.0",
  "parquet_path": "models/minilm/weights.parquet",
  "tokenizer_path": "models/minilm/tokenizer"
}