{
  "model_name": "models/raw/clip-vit-base-patch32",
  "model_type": "CLIP Vision Transformer",
  "architecture": "ViT",
  "config": {
    "image_size": 224,
    "patch_size": 32,
    "hidden_size": 768,
    "num_layers": 12,
    "num_attention_heads": 12,
    "intermediate_size": 3072,
    "layer_norm_eps": 1e-05,
    "projection_dim": 512
  },
  "conversion_config": {
    "compression": "zstd",
    "compression_level": 3,
    "use_float16": true,
    "extract_tokenizer": true,
    "validate_output": true
  },
  "total_parameters": 87849216,
  "num_weight_tensors": 200,
  "layer_info": {
    "vision_model.embeddings.class_embedding": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.embeddings.patch_embedding.weight": {
      "shape": [
        768,
        3,
        32,
        32
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.embeddings.position_embedding.weight": {
      "shape": [
        50,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 38400
    },
    "vision_model.pre_layrnorm.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.pre_layrnorm.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.0.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.0.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.0.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.0.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.0.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.0.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.0.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.0.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.1.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.1.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.1.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.1.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.1.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.1.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.1.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.1.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.2.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.2.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.2.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.2.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.2.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.2.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.2.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.2.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.3.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.3.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.3.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.3.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.3.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.3.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.3.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.3.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.4.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.4.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.4.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.4.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.4.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.4.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.4.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.4.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.5.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.5.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.5.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.5.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.5.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.5.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.5.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.5.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.6.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.6.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.6.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.6.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.6.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.6.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.6.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.6.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.7.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.7.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.7.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.7.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.7.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.7.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.7.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.7.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.8.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.8.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.8.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.8.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.8.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.8.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.8.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.8.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.9.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.9.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.9.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.9.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.9.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.9.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.9.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.9.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.10.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.10.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.10.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.10.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.10.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.10.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.10.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.10.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.self_attn.k_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.11.self_attn.k_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.self_attn.v_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.11.self_attn.v_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.self_attn.q_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.11.self_attn.q_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.self_attn.out_proj.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 589824
    },
    "vision_model.encoder.layers.11.self_attn.out_proj.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.layer_norm1.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.layer_norm1.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.mlp.fc1.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.11.mlp.fc1.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 3072
    },
    "vision_model.encoder.layers.11.mlp.fc2.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float16",
      "num_params": 2359296
    },
    "vision_model.encoder.layers.11.mlp.fc2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.layer_norm2.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.encoder.layers.11.layer_norm2.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.post_layernorm.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "vision_model.post_layernorm.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float16",
      "num_params": 768
    },
    "visual_projection.weight": {
      "shape": [
        512,
        768
      ],
      "dtype": "torch.float16",
      "num_params": 393216
    }
  },
  "parquet_path": "llm_compression/models/clip-vit-base-patch32/weights.parquet",
  "converted_at": "2026-02-20T06:58:52.133363",
  "converter_version": "0.2.0"
}