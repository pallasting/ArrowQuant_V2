"""
Cognitive Loop End-to-End Demo

å±•ç¤ºå®Œæ•´çš„è‡ªç»„ç»‡è®¤çŸ¥å¾ªç¯ç³»ç»Ÿï¼š
1. è®°å¿†å‹ç¼©ä¸æ„å»º
2. æŸ¥è¯¢å¤„ç†
3. å­¦ä¹ æ¼”åŒ–
4. è´¨é‡æ”¹è¿›
5. ç½‘ç»œç»Ÿè®¡
"""

import asyncio
import numpy as np
from datetime import datetime

from llm_compression import (
    LLMClient,
    LLMCompressor,
    LLMReconstructor,
    MemoryPrimitive,
    CognitiveLoop,
    MultiModalExpressor,
    InternalFeedbackSystem,
    ModelSelector
)


# æµ‹è¯•æ–‡æœ¬
SAMPLE_TEXTS = [
    "Python is a high-level programming language known for its simplicity and readability.",
    "Python is widely used in web development, data science, and artificial intelligence.",
    "Machine learning is a subset of AI that enables systems to learn from data.",
    "Deep learning uses neural networks with multiple layers to process complex patterns.",
    "Natural language processing helps computers understand and generate human language.",
]


async def compress_and_build_network(texts, llm_client, compressor):
    """å‹ç¼©æ–‡æœ¬å¹¶æ„å»ºè®°å¿†ç½‘ç»œ"""
    print("\n" + "="*60)
    print("ğŸ“¦ æ­¥éª¤1: å‹ç¼©æ–‡æœ¬å¹¶æ„å»ºè®°å¿†ç½‘ç»œ")
    print("="*60)
    
    memories = []
    for i, text in enumerate(texts):
        print(f"\nå‹ç¼©æ–‡æœ¬ {i+1}/{len(texts)}...")
        print(f"åŸæ–‡: {text[:60]}...")
        
        # å‹ç¼©
        compressed = await compressor.compress(text)
        
        # åˆ›å»ºembeddingï¼ˆç®€åŒ–ï¼šä½¿ç”¨éšæœºå‘é‡ï¼‰
        # å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨çœŸå®çš„embeddingæ¨¡å‹
        embedding = np.random.randn(384)
        embedding = embedding / np.linalg.norm(embedding)
        
        # åˆ›å»ºMemoryPrimitive
        memory = MemoryPrimitive(
            id=f"mem_{i}",
            content=compressed,
            embedding=embedding
        )
        
        memories.append(memory)
        
        print(f"âœ… å‹ç¼©å®Œæˆ: {compressed.compression_metadata.original_size}B â†’ "
              f"{compressed.compression_metadata.compressed_size}B "
              f"(å‹ç¼©ç‡: {compressed.compression_metadata.compression_ratio:.1f}x)")
    
    print(f"\nâœ… å…±åˆ›å»º {len(memories)} ä¸ªè®°å¿†å•å…ƒ")
    return memories


async def demonstrate_cognitive_loop(memories, llm_client):
    """æ¼”ç¤ºè®¤çŸ¥å¾ªç¯"""
    print("\n" + "="*60)
    print("ğŸ§  æ­¥éª¤2: æ¼”ç¤ºè®¤çŸ¥å¾ªç¯")
    print("="*60)
    
    # åˆ›å»ºç»„ä»¶
    reconstructor = LLMReconstructor(llm_client)
    expressor = MultiModalExpressor(llm_client, reconstructor)
    feedback = InternalFeedbackSystem(llm_client, reconstructor)
    
    # åˆ›å»ºè®¤çŸ¥å¾ªç¯
    loop = CognitiveLoop(
        expressor=expressor,
        feedback=feedback,
        quality_threshold=0.85,
        max_corrections=2,
        learning_rate=0.1
    )
    
    # æ·»åŠ è®°å¿†åˆ°ç½‘ç»œ
    for memory in memories:
        loop.add_memory(memory)
    
    print(f"\nâœ… è®¤çŸ¥å¾ªç¯å·²åˆå§‹åŒ–")
    print(f"   - è®°å¿†æ•°é‡: {len(memories)}")
    print(f"   - è´¨é‡é˜ˆå€¼: {loop.quality_threshold}")
    print(f"   - æœ€å¤§çº æ­£æ¬¡æ•°: {loop.max_corrections}")
    
    return loop


async def process_queries(loop, queries):
    """å¤„ç†æŸ¥è¯¢å¹¶å±•ç¤ºç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ’¬ æ­¥éª¤3: å¤„ç†æŸ¥è¯¢")
    print("="*60)
    
    for i, query in enumerate(queries):
        print(f"\n{'â”€'*60}")
        print(f"æŸ¥è¯¢ {i+1}: {query}")
        print(f"{'â”€'*60}")
        
        # åˆ›å»ºæŸ¥è¯¢embeddingï¼ˆç®€åŒ–ï¼‰
        query_embedding = np.random.randn(384)
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        # å¤„ç†æŸ¥è¯¢
        print("\nğŸ”„ æ‰§è¡Œè®¤çŸ¥å¾ªç¯...")
        result = await loop.process(
            query=query,
            query_embedding=query_embedding,
            max_memories=3
        )
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š ç»“æœ:")
        print(f"   - è¾“å‡º: {result.output[:100]}...")
        print(f"   - è´¨é‡è¯„åˆ†: {result.quality.overall:.2f}")
        print(f"   - ä½¿ç”¨è®°å¿†: {len(result.memories_used)} ä¸ª")
        print(f"   - çº æ­£æ¬¡æ•°: {result.corrections_applied}")
        print(f"   - å‘ç”Ÿå­¦ä¹ : {'æ˜¯' if result.learning_occurred else 'å¦'}")
        
        print(f"\nğŸ“ˆ è´¨é‡è¯¦æƒ…:")
        print(f"   - ä¸€è‡´æ€§: {result.quality.consistency:.2f}")
        print(f"   - å®Œæ•´æ€§: {result.quality.completeness:.2f}")
        print(f"   - å‡†ç¡®æ€§: {result.quality.accuracy:.2f}")
        print(f"   - è¿è´¯æ€§: {result.quality.coherence:.2f}")


def show_network_evolution(loop, initial_stats):
    """å±•ç¤ºç½‘ç»œæ¼”åŒ–"""
    print("\n" + "="*60)
    print("ğŸ“ˆ æ­¥éª¤4: ç½‘ç»œæ¼”åŒ–ç»Ÿè®¡")
    print("="*60)
    
    final_stats = loop.get_network_stats()
    
    print(f"\nåˆå§‹çŠ¶æ€:")
    print(f"   - è®°å¿†æ•°é‡: {initial_stats['total_memories']}")
    print(f"   - æ€»è¿æ¥æ•°: {initial_stats['total_connections']}")
    print(f"   - å¹³å‡è¿æ¥: {initial_stats['avg_connections']:.2f}")
    print(f"   - å¹³å‡æˆåŠŸç‡: {initial_stats['avg_success_rate']:.2f}")
    
    print(f"\næœ€ç»ˆçŠ¶æ€:")
    print(f"   - è®°å¿†æ•°é‡: {final_stats['total_memories']}")
    print(f"   - æ€»è¿æ¥æ•°: {final_stats['total_connections']}")
    print(f"   - å¹³å‡è¿æ¥: {final_stats['avg_connections']:.2f}")
    print(f"   - å¹³å‡æˆåŠŸç‡: {final_stats['avg_success_rate']:.2f}")
    
    print(f"\nå˜åŒ–:")
    print(f"   - è¿æ¥å¢é•¿: {final_stats['total_connections'] - initial_stats['total_connections']}")
    print(f"   - å¹³å‡è¿æ¥å¢é•¿: {final_stats['avg_connections'] - initial_stats['avg_connections']:.2f}")
    print(f"   - æˆåŠŸç‡å˜åŒ–: {final_stats['avg_success_rate'] - initial_stats['avg_success_rate']:.2f}")


async def main():
    """ä¸»æ¼”ç¤ºæµç¨‹"""
    print("\n" + "="*60)
    print("ğŸš€ Phase 2.0 è®¤çŸ¥é—­ç¯ç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
    print("\nåˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
    llm_client = LLMClient(
        endpoint="http://localhost:11434"
    )
    
    # æµ‹è¯•è¿æ¥
    try:
        response = await llm_client.generate("test")
        print("âœ… LLMè¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ LLMè¿æ¥å¤±è´¥: {e}")
        print("\nè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ:")
        print("  ollama serve")
        print("  ollama pull qwen2.5:7b-instruct")
        return
    
    # åˆ›å»ºå‹ç¼©å™¨
    model_selector = ModelSelector()
    compressor = LLMCompressor(llm_client, model_selector)
    
    # æ­¥éª¤1: å‹ç¼©å¹¶æ„å»ºç½‘ç»œ
    memories = await compress_and_build_network(SAMPLE_TEXTS, llm_client, compressor)
    
    # æ­¥éª¤2: åˆ›å»ºè®¤çŸ¥å¾ªç¯
    loop = await demonstrate_cognitive_loop(memories, llm_client)
    
    # è®°å½•åˆå§‹ç»Ÿè®¡
    initial_stats = loop.get_network_stats()
    
    # æ­¥éª¤3: å¤„ç†æŸ¥è¯¢
    queries = [
        "What is Python used for?",
        "Explain machine learning",
        "How does deep learning work?"
    ]
    
    await process_queries(loop, queries)
    
    # æ­¥éª¤4: å±•ç¤ºç½‘ç»œæ¼”åŒ–
    show_network_evolution(loop, initial_stats)
    
    print("\n" + "="*60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("\nå…³é”®æˆå°±:")
    print("  âœ… è®°å¿†å‹ç¼©ä¸æ„å»º")
    print("  âœ… å®Œæ•´è®¤çŸ¥å¾ªç¯")
    print("  âœ… è‡ªæˆ‘çº æ­£æœºåˆ¶")
    print("  âœ… Hebbianå­¦ä¹ ")
    print("  âœ… ç½‘ç»œè‡ªç»„ç»‡")
    print("\nè¿™æ˜¯ä¸€ä¸ªçœŸæ­£çš„è‡ªç»„ç»‡è®¤çŸ¥ç³»ç»Ÿï¼ğŸ§ âœ¨")


if __name__ == "__main__":
    asyncio.run(main())
