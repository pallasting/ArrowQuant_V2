#!/usr/bin/env python3
"""
ä¼šè¯å‹ç¼©éªŒè¯å™¨
ä½¿ç”¨çœŸå®çš„ Windsurf ä¼šè¯æ•°æ®éªŒè¯å‹ç¼©ç³»ç»Ÿ
"""

import os
import re
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import asyncio

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.insert(0, '/Media/Ubuntu/Documents/Surface-Memory/Documents/ai-os-memory')

from llm_compression.compressor import LLMCompressor
from llm_compression.reconstructor import LLMReconstructor
from llm_compression.quality_evaluator import QualityEvaluator
from llm_compression.model_selector import ModelSelector, MemoryType, QualityLevel


class ConversationValidator:
    """ä¼šè¯å‹ç¼©éªŒè¯å™¨"""
    
    def __init__(
        self,
        data_dir: str = "/Data/CascadeProjects/TalkingWithU",
        model_name: str = "gemma3",  # é»˜è®¤ä½¿ç”¨ Gemma 3 4B
        output_dir: str = "validation_results"
    ):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        print(f"åˆå§‹åŒ–å‹ç¼©ç³»ç»Ÿ (æ¨¡å‹: {model_name})...")
        self.model_name = model_name
        
        # åˆ›å»ºä¾èµ–ç»„ä»¶
        from llm_compression.llm_client import LLMClient
        from llm_compression.config import load_config
        
        config = load_config()
        
        # ä½¿ç”¨æœ¬åœ°GPUæ¨¡å‹ï¼ˆOllama + Vulkanï¼‰
        endpoint = 'http://localhost:11434'
        print(f"ä½¿ç”¨æœ¬åœ°GPUæ¨¡å‹: {endpoint} (VulkanåŠ é€Ÿ)")
        
        self.llm_client = LLMClient(endpoint)
        self.model_selector = ModelSelector(
            cloud_endpoint=config.llm.cloud_endpoint,
            ollama_endpoint='http://localhost:11434',
            prefer_local=True  # ä¼˜å…ˆæœ¬åœ°GPU
        )
        
        self.compressor = LLMCompressor(self.llm_client, self.model_selector)
        self.reconstructor = LLMReconstructor(self.llm_client, quality_threshold=0.85)
        self.evaluator = QualityEvaluator()
        
        # ç»“æœå­˜å‚¨
        self.results = []
        self.errors = []
    
    def load_conversations(self) -> List[Path]:
        """åŠ è½½æ‰€æœ‰ä¼šè¯æ–‡ä»¶"""
        files = sorted(self.data_dir.glob("*.txt.md"))
        print(f"\næ‰¾åˆ° {len(files)} ä¸ªä¼šè¯æ–‡ä»¶")
        return files
    
    def parse_conversation(self, file_path: Path) -> List[Dict[str, str]]:
        """è§£æä¼šè¯æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"  è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return []
        
        messages = []
        lines = content.split('\n')
        
        current_role = None
        current_content = []
        
        for line in lines:
            # æ£€æµ‹è§’è‰²åˆ‡æ¢
            if line.startswith('Assistant') or line.startswith('assistant'):
                if current_role and current_content:
                    messages.append({
                        'role': current_role,
                        'content': '\n'.join(current_content).strip()
                    })
                current_role = 'assistant'
                current_content = []
            elif line.startswith('Human') or line.startswith('human') or line.startswith('User'):
                if current_role and current_content:
                    messages.append({
                        'role': current_role,
                        'content': '\n'.join(current_content).strip()
                    })
                current_role = 'user'
                current_content = []
            elif line.strip():
                # è·³è¿‡æ—¶é—´æˆ³è¡Œ
                if not re.match(r'^\d{4}-\d{2}-\d{2}', line):
                    current_content.append(line)
        
        # æ·»åŠ æœ€åä¸€æ¡æ¶ˆæ¯
        if current_role and current_content:
            messages.append({
                'role': current_role,
                'content': '\n'.join(current_content).strip()
            })
        
        # è¿‡æ»¤ç©ºæ¶ˆæ¯
        messages = [m for m in messages if m['content'].strip()]
        
        return messages
    
    def format_messages(self, messages: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºæ–‡æœ¬"""
        return '\n\n'.join([
            f"[{msg['role']}]: {msg['content']}"
            for msg in messages
        ])
    
    async def validate_file(self, file_path: Path) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªæ–‡ä»¶"""
        print(f"\n{'='*60}")
        print(f"å¤„ç†: {file_path.name}")
        print(f"{'='*60}")
        
        result = {
            'file': file_path.name,
            'timestamp': datetime.now().isoformat(),
            'model': self.model_name,
            'success': False
        }
        
        try:
            # 1. è§£æä¼šè¯
            messages = self.parse_conversation(file_path)
            if not messages:
                print("  âš ï¸  æ— æ³•è§£æä¼šè¯å†…å®¹")
                result['error'] = "æ— æ³•è§£æä¼šè¯"
                self.errors.append(result)
                return result
            
            result['message_count'] = len(messages)
            print(f"  æ¶ˆæ¯æ•°: {len(messages)}")
            
            # 2. æ ¼å¼åŒ–æ–‡æœ¬
            text = self.format_messages(messages)
            original_length = len(text)
            result['original_length'] = original_length
            result['original_chars'] = original_length
            print(f"  åŸå§‹é•¿åº¦: {original_length:,} å­—ç¬¦")
            
            # 3. å‹ç¼©
            print(f"  å‹ç¼©ä¸­...")
            start_time = time.time()
            
            compressed = await self.compressor.compress(text)
            
            compress_time = time.time() - start_time
            result['compress_time'] = compress_time
            
            # è®¡ç®—å‹ç¼©åå¤§å°ï¼ˆsummary_hash + entities + diff_dataï¼‰
            compressed_size = compressed.compression_metadata.compressed_size
            result['compressed_length'] = compressed_size
            result['compressed_chars'] = compressed_size
            
            compression_ratio = compressed.compression_metadata.compression_ratio
            result['compression_ratio'] = compression_ratio
            
            print(f"  âœ… å‹ç¼©å®Œæˆ")
            print(f"     å‹ç¼©å: {compressed_size:,} å­—èŠ‚")
            print(f"     å‹ç¼©æ¯”: {compression_ratio:.2f}x")
            print(f"     è€—æ—¶: {compress_time:.2f}s")
            
            # 4. é‡æ„
            print(f"  é‡æ„ä¸­...")
            start_time = time.time()
            
            reconstructed = await self.reconstructor.reconstruct(compressed)
            
            reconstruct_time = time.time() - start_time
            result['reconstruct_time'] = reconstruct_time
            result['reconstructed_length'] = len(reconstructed.full_text)
            
            print(f"  âœ… é‡æ„å®Œæˆ")
            print(f"     è€—æ—¶: {reconstruct_time:.2f}s")
            
            # 5. è´¨é‡è¯„ä¼°
            print(f"  è¯„ä¼°è´¨é‡...")
            quality = self.evaluator.evaluate(
                text, 
                reconstructed.full_text,
                compressed_size=compressed.compression_metadata.compressed_size,
                reconstruction_latency_ms=reconstruct_time * 1000
            )
            
            result['quality_score'] = quality.overall_score
            result['semantic_similarity'] = quality.semantic_similarity
            result['entity_accuracy'] = getattr(quality, 'entity_accuracy', 0.0)
            
            print(f"  âœ… è´¨é‡è¯„ä¼°å®Œæˆ")
            print(f"     æ€»åˆ†: {quality.overall_score:.3f}")
            print(f"     è¯­ä¹‰ç›¸ä¼¼åº¦: {quality.semantic_similarity:.3f}")
            
            # 6. æ£€æŸ¥å…³é”®ä¿¡æ¯ä¿ç•™
            sample_keywords = self._extract_keywords(text)
            preserved_keywords = sum(1 for kw in sample_keywords if kw in reconstructed.full_text)
            keyword_retention = preserved_keywords / len(sample_keywords) if sample_keywords else 0
            result['keyword_retention'] = keyword_retention
            
            print(f"     å…³é”®è¯ä¿ç•™: {keyword_retention:.1%} ({preserved_keywords}/{len(sample_keywords)})")
            
            result['success'] = True
            self.results.append(result)
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            self._save_detail(file_path.stem, {
                'original': text[:500] + '...' if len(text) > 500 else text,
                'compressed_hash': compressed.summary_hash,
                'reconstructed': reconstructed.full_text[:500] + '...' if len(reconstructed.full_text) > 500 else reconstructed.full_text,
                'metrics': result
            })
            
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            result['error'] = str(e)
            self.errors.append(result)
        
        return result
    
    def _extract_keywords(self, text: str, max_keywords: int = 20) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰"""
        # æå–é•¿åº¦ > 3 çš„ä¸­æ–‡è¯å’Œè‹±æ–‡å•è¯
        words = re.findall(r'[\u4e00-\u9fff]{2,}|[a-zA-Z]{4,}', text)
        # å»é‡å¹¶å–å‰ N ä¸ª
        unique_words = list(dict.fromkeys(words))[:max_keywords]
        return unique_words
    
    def _save_detail(self, name: str, data: Dict[str, Any]):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        detail_file = self.output_dir / f"{name}_detail.json"
        with open(detail_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    async def validate_all(self):
        """éªŒè¯æ‰€æœ‰æ–‡ä»¶"""
        files = self.load_conversations()
        
        if not files:
            print("æ²¡æœ‰æ‰¾åˆ°ä¼šè¯æ–‡ä»¶")
            return
        
        print(f"\nå¼€å§‹éªŒè¯ {len(files)} ä¸ªæ–‡ä»¶...")
        print(f"æ¨¡å‹: {self.model_name}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        for i, file_path in enumerate(files, 1):
            print(f"\n[{i}/{len(files)}]")
            await self.validate_file(file_path)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        if not self.results:
            print("\næ²¡æœ‰æˆåŠŸçš„éªŒè¯ç»“æœ")
            return
        
        print(f"\n{'='*60}")
        print("éªŒè¯æŠ¥å‘Šæ±‡æ€»")
        print(f"{'='*60}")
        
        # ç»Ÿè®¡
        total_files = len(self.results) + len(self.errors)
        success_count = len(self.results)
        error_count = len(self.errors)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"  æˆåŠŸ: {success_count} ({success_count/total_files*100:.1f}%)")
        print(f"  å¤±è´¥: {error_count} ({error_count/total_files*100:.1f}%)")
        
        if not self.results:
            return
        
        # è®¡ç®—å¹³å‡å€¼
        avg_compression_ratio = sum(r['compression_ratio'] for r in self.results) / len(self.results)
        avg_compress_time = sum(r['compress_time'] for r in self.results) / len(self.results)
        avg_reconstruct_time = sum(r['reconstruct_time'] for r in self.results) / len(self.results)
        avg_quality = sum(r['quality_score'] for r in self.results) / len(self.results)
        avg_similarity = sum(r['semantic_similarity'] for r in self.results) / len(self.results)
        avg_keyword_retention = sum(r.get('keyword_retention', 0) for r in self.results) / len(self.results)
        
        # è®¡ç®—ååé‡
        total_time = sum(r['compress_time'] + r['reconstruct_time'] for r in self.results)
        throughput = (len(self.results) * 60) / total_time if total_time > 0 else 0
        
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å¹³å‡å‹ç¼©æ¯”: {avg_compression_ratio:.2f}x")
        print(f"  å¹³å‡å‹ç¼©è€—æ—¶: {avg_compress_time:.2f}s")
        print(f"  å¹³å‡é‡æ„è€—æ—¶: {avg_reconstruct_time:.2f}s")
        print(f"  ååé‡: {throughput:.1f} æ–‡ä»¶/åˆ†é’Ÿ")
        
        print(f"\nğŸ¯ è´¨é‡æŒ‡æ ‡:")
        print(f"  å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.3f}")
        print(f"  å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
        print(f"  å¹³å‡å…³é”®è¯ä¿ç•™: {avg_keyword_retention:.1%}")
        
        # ç›®æ ‡å¯¹æ¯”
        print(f"\nâœ… ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        print(f"  å‹ç¼©æ¯” > 10x: {'âœ…' if avg_compression_ratio > 10 else 'âŒ'} ({avg_compression_ratio:.2f}x)")
        print(f"  å‹ç¼©å»¶è¿Ÿ < 10s: {'âœ…' if avg_compress_time < 10 else 'âŒ'} ({avg_compress_time:.2f}s)")
        print(f"  é‡æ„å»¶è¿Ÿ < 500ms: {'âœ…' if avg_reconstruct_time < 0.5 else 'âŒ'} ({avg_reconstruct_time*1000:.0f}ms)")
        print(f"  è´¨é‡ > 0.85: {'âœ…' if avg_quality > 0.85 else 'âŒ'} ({avg_quality:.3f})")
        print(f"  ååé‡ > 10/min: {'âœ…' if throughput > 10 else 'âŒ'} ({throughput:.1f}/min)")
        
        # ä¿å­˜æŠ¥å‘Š
        report = {
            'summary': {
                'model': self.model_name,
                'timestamp': datetime.now().isoformat(),
                'total_files': total_files,
                'success_count': success_count,
                'error_count': error_count,
                'avg_compression_ratio': avg_compression_ratio,
                'avg_compress_time': avg_compress_time,
                'avg_reconstruct_time': avg_reconstruct_time,
                'avg_quality_score': avg_quality,
                'avg_semantic_similarity': avg_similarity,
                'avg_keyword_retention': avg_keyword_retention,
                'throughput': throughput
            },
            'results': self.results,
            'errors': self.errors
        }
        
        report_file = self.output_dir / f"validation_report_{self.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        self._generate_markdown_report(report, report_file.with_suffix('.md'))
    
    def _generate_markdown_report(self, report: Dict[str, Any], output_file: Path):
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        summary = report['summary']
        
        md = f"""# ä¼šè¯å‹ç¼©éªŒè¯æŠ¥å‘Š

**æ¨¡å‹**: {summary['model']}  
**æ—¶é—´**: {summary['timestamp']}  
**æ•°æ®æº**: Windsurf ä¼šè¯è®°å½•

---

## æ‰§è¡Œæ‘˜è¦

### æ€»ä½“ç»Ÿè®¡

- **æ€»æ–‡ä»¶æ•°**: {summary['total_files']}
- **æˆåŠŸ**: {summary['success_count']} ({summary['success_count']/summary['total_files']*100:.1f}%)
- **å¤±è´¥**: {summary['error_count']} ({summary['error_count']/summary['total_files']*100:.1f}%)

### æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç»“æœ | ç›®æ ‡ | çŠ¶æ€ |
|------|------|------|------|
| å¹³å‡å‹ç¼©æ¯” | {summary['avg_compression_ratio']:.2f}x | > 10x | {'âœ…' if summary['avg_compression_ratio'] > 10 else 'âŒ'} |
| å¹³å‡å‹ç¼©è€—æ—¶ | {summary['avg_compress_time']:.2f}s | < 10s | {'âœ…' if summary['avg_compress_time'] < 10 else 'âŒ'} |
| å¹³å‡é‡æ„è€—æ—¶ | {summary['avg_reconstruct_time']*1000:.0f}ms | < 500ms | {'âœ…' if summary['avg_reconstruct_time'] < 0.5 else 'âŒ'} |
| ååé‡ | {summary['throughput']:.1f}/min | > 10/min | {'âœ…' if summary['throughput'] > 10 else 'âŒ'} |

### è´¨é‡æŒ‡æ ‡

| æŒ‡æ ‡ | ç»“æœ | ç›®æ ‡ | çŠ¶æ€ |
|------|------|------|------|
| å¹³å‡è´¨é‡åˆ†æ•° | {summary['avg_quality_score']:.3f} | > 0.85 | {'âœ…' if summary['avg_quality_score'] > 0.85 else 'âŒ'} |
| å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦ | {summary['avg_semantic_similarity']:.3f} | > 0.85 | {'âœ…' if summary['avg_semantic_similarity'] > 0.85 else 'âŒ'} |
| å¹³å‡å…³é”®è¯ä¿ç•™ | {summary['avg_keyword_retention']:.1%} | > 90% | {'âœ…' if summary['avg_keyword_retention'] > 0.9 else 'âŒ'} |

---

## è¯¦ç»†ç»“æœ

"""
        
        for i, result in enumerate(report['results'], 1):
            md += f"""
### {i}. {result['file']}

- **æ¶ˆæ¯æ•°**: {result['message_count']}
- **åŸå§‹é•¿åº¦**: {result['original_length']:,} å­—ç¬¦
- **å‹ç¼©å**: {result['compressed_length']:,} å­—ç¬¦
- **å‹ç¼©æ¯”**: {result['compression_ratio']:.2f}x
- **å‹ç¼©è€—æ—¶**: {result['compress_time']:.2f}s
- **é‡æ„è€—æ—¶**: {result['reconstruct_time']:.2f}s
- **è´¨é‡åˆ†æ•°**: {result['quality_score']:.3f}
- **è¯­ä¹‰ç›¸ä¼¼åº¦**: {result['semantic_similarity']:.3f}
- **å…³é”®è¯ä¿ç•™**: {result.get('keyword_retention', 0):.1%}

"""
        
        if report['errors']:
            md += "\n---\n\n## é”™è¯¯è®°å½•\n\n"
            for error in report['errors']:
                md += f"- **{error['file']}**: {error.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(md)
        
        print(f"ğŸ“„ Markdown æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¼šè¯å‹ç¼©éªŒè¯å™¨')
    parser.add_argument('--data-dir', default='/Data/CascadeProjects/TalkingWithU',
                        help='ä¼šè¯æ•°æ®ç›®å½•')
    parser.add_argument('--model', default='gemma3',
                        choices=['gemma3', 'qwen2.5', 'tinyllama', 'cloud'],
                        help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--output-dir', default='validation_results',
                        help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    print("="*60)
    print("ä¼šè¯å‹ç¼©éªŒè¯å™¨")
    print("="*60)
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"æ¨¡å‹: {args.model}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    validator = ConversationValidator(
        data_dir=args.data_dir,
        model_name=args.model,
        output_dir=args.output_dir
    )
    
    await validator.validate_all()
    
    print("\nâœ… éªŒè¯å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
